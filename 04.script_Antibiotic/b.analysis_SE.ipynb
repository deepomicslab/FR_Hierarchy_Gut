{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script check SE difference of control and exposed group at each clsuter/super-cluter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script check SE difference of control and exposed group at each clsuter/super-cluter.\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import os\n",
    "import se\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "import matplotlib.pyplot as plt\n",
    "import tree_util\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.multitest import fdrcorrection as fdr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute FR network\n",
    "def fr(d_df, profile, sname):\n",
    "    sp_list = list(profile.columns)\n",
    "    sp_d_df = d_df.loc[sp_list, sp_list]\n",
    "    #print(profile)\n",
    "    sp_profile = np.array(profile.loc[sname, sp_list])\n",
    "    value = np.dot(sp_profile.reshape(len(sp_profile), 1),sp_profile.reshape(1, len(sp_profile)))\n",
    "    width = value.shape[0]\n",
    "    cor_df = np.ones(shape=(width, width)) - sp_d_df.values\n",
    "    for i in range(width):\n",
    "        cor_df[i][i] = 0\n",
    "    value = np.multiply(value, cor_df)\n",
    "    df = pd.DataFrame(data= value, index=sp_list, columns=sp_list)\n",
    "    return df\n",
    "\n",
    "# compute SE for all sample\n",
    "def multisample_se(profile, d_df, parent_dict, node_leaves, child_dict, direct_children_dict, param):\n",
    "    result = pd.DataFrame(index=profile.index, columns=(list(node_leaves.keys()) + ['nroot']))\n",
    "    for sname in profile.index:\n",
    "       # print(sname)\n",
    "       edge_df = fr(d_df, profile, sname)\n",
    "       tmp = se.subtree_se_adj(edge_df, parent_dict, node_leaves, child_dict, direct_children_dict, param)\n",
    "       for node in tmp.keys():\n",
    "           value = tmp[node]\n",
    "           result.loc[sname, node] = value\n",
    "    if 'nroot' in result.columns:\n",
    "        result.drop('nroot', axis=1, inplace=True)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '../result/Anti/SE'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# load tree and construct tree with newick string\n",
    "with open('../result/GCN_fix_tree/renamed_GCN_tree.newick') as fp:\n",
    "    newick_tree = fp.read()\n",
    "    # newick_tree = newick_tree.replace('-', '_')\n",
    "\n",
    "json_tree = tree_util.parse(newick_tree)\n",
    "largest = {'largest': 0}\n",
    "leaf_list, l = tree_util.recu_compute(json_tree, 0, largest)\n",
    "largest_level = largest['largest']\n",
    "nlayer = largest_level\n",
    "layer_leaves_dict = tree_util.make_layer_dict(nlayer)\n",
    "tree_util.recu_layer(json_tree, layer_leaves_dict)\n",
    "tree_util.to_layer_leaves(layer_leaves_dict, nlayer)\n",
    "leaves_dict = copy.deepcopy(layer_leaves_dict)\n",
    "parent_dict = {}\n",
    "tree_util.parents(json_tree, parent_dict)\n",
    "node_leaves = {}\n",
    "for level in layer_leaves_dict.keys():\n",
    "    for node, sp_list in layer_leaves_dict[level].items():\n",
    "        if node in node_leaves.keys():\n",
    "            continue\n",
    "        node_leaves[node] = copy.deepcopy(sp_list)\n",
    "subtree_nodes = {}\n",
    "for l in leaf_list:\n",
    "    parent = parent_dict[l]\n",
    "    if parent not in subtree_nodes.keys():\n",
    "        subtree_nodes[parent] = []\n",
    "    subtree_nodes[parent].append(l)\n",
    "\n",
    "for node in node_leaves.keys():\n",
    "    parent = parent_dict[node]\n",
    "    if parent not in subtree_nodes.keys():\n",
    "        subtree_nodes[parent] = []\n",
    "    subtree_nodes[parent] += subtree_nodes[node]\n",
    "    subtree_nodes[parent].append(node)\n",
    "\n",
    "for node in subtree_nodes.keys():\n",
    "    subtree_nodes[node].append(node)\n",
    "\n",
    "direct_children_dict = {}\n",
    "for node, parent in parent_dict.items():\n",
    "    if parent not in direct_children_dict:\n",
    "        direct_children_dict[parent] = []\n",
    "    direct_children_dict[parent].append(node)\n",
    "\n",
    "node_leaves['root'] = list(leaf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = 0.5\n",
    "p_cutoff = 0.05\n",
    "\n",
    "# load distance, abundance profile and metadata\n",
    "d_df = pd.read_csv('../data/sp_d.tsv', sep='\\t', header=0, index_col=0)\n",
    "metadata = pd.read_csv('../data/Anti/metadata.csv', sep=',', index_col=None, header=0)\n",
    "abd = pd.read_csv('../data/Anti/abd.tsv', sep='\\t', header=0, index_col=0)\n",
    "metadata = metadata[['Sample_Name', 'Timepoint', 'type']]\n",
    "metadata = metadata.drop_duplicates(inplace=False)\n",
    "sname_tmp = 'P{}E{}'\n",
    "abd_name_dict = {}\n",
    "for idx in abd.index:\n",
    "    abd_name_dict[idx] = idx.split('|')[-1].replace('_', '-')\n",
    "abd = abd.rename(index=abd_name_dict)    \n",
    "name_dict = {}\n",
    "for sp in d_df.columns:\n",
    "        name_dict[sp] = sp.replace('_', '-')\n",
    "d_df = d_df.rename(columns=name_dict, index=name_dict)\n",
    "abd = abd.T/100\n",
    "sub_ids = sorted(list(set(metadata['Sample_Name'])))\n",
    "days = sorted(list(set(metadata['Timepoint'])))\n",
    "phenos = list(set(metadata['type']))\n",
    "profile = abd[list(set(abd.columns).intersection(set(d_df.columns)))]\n",
    "\n",
    "# compute SE for all samples\n",
    "se_result = multisample_se(profile, d_df, parent_dict, node_leaves, subtree_nodes, direct_children_dict, param)\n",
    "se_result.to_csv(os.path.join(outdir, 'se_df.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "p1 = phenos[1]\n",
    "p0 = phenos[0]\n",
    "\n",
    "# differential test\n",
    "p_df = pd.DataFrame(columns=days, index=list(node_leaves.keys()))\n",
    "valid_dict = {}\n",
    "for k in node_leaves.keys():\n",
    "    metadata[k] = list(se_result.loc[list(metadata['Sample_Name']), k] )\n",
    "    for j in days:\n",
    "        selected0 = list(metadata[(metadata['Timepoint'] == j) & (metadata['type'] == p0)]['Sample_Name'])\n",
    "        selected1 = list(metadata[(metadata['Timepoint'] == j) & (metadata['type'] == p1)]['Sample_Name'])\n",
    "        selected_v0 = se_result.loc[selected0, k]\n",
    "        selected_v1 = se_result.loc[selected1, k]\n",
    "        if (list(selected_v0).count(0) > len(list(selected_v0))*0.8) and (list(selected_v1).count(0) > len(list(selected_v1))*0.8):\n",
    "            continue\n",
    "        if j not in valid_dict.keys():\n",
    "            valid_dict[j] = []\n",
    "        valid_dict[j].append(k)\n",
    "        t, p = mannwhitneyu(list(selected_v0), list(selected_v1))\n",
    "        p_df.loc[k, j] = p\n",
    "\n",
    "# adjust with FDR  \n",
    "p_df_adj = pd.DataFrame(columns=p_df.columns, index=list(node_leaves.keys()))\n",
    "for d in days:\n",
    "    p_values = p_df.loc[valid_dict[d], d]\n",
    "    p_adj = fdr(p_values, p_cutoff)[1]\n",
    "    for i, cluster in enumerate(valid_dict[d]):\n",
    "        p_df_adj.loc[cluster, d] = p_adj[i]\n",
    "p_df_adj.fillna(np.nan, inplace=True)\n",
    "p_df_adj.to_csv(os.path.join(outdir, 'p_value.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make boxplot\n",
    "for k in node_leaves.keys():\n",
    "    metadata[k] = list(se_result.loc[list(metadata['Sample_Name']), k] )\n",
    "    max_v = max(list(metadata[k]))\n",
    "    show = False\n",
    "    for j in days:\n",
    "        if k in valid_dict[j]:\n",
    "            show = True\n",
    "    if not show:\n",
    "        continue\n",
    "    sns.boxplot(x='Timepoint', y=k, hue='type', data=metadata)\n",
    "    for i, t in enumerate(days):\n",
    "        if pd.isna(p_df_adj.loc[k, t]):\n",
    "            plt.text(i, max_v*1.2, 'X', ha='center', va='bottom', fontsize=20, color='black')\n",
    "\n",
    "        if p_df_adj.loc[k, t] < p_cutoff:\n",
    "            plt.text(i, max_v*1.2, '**', ha='center', va='bottom', fontsize=20, color='r')\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.ylim(-0.1*max_v, max_v*1.5)\n",
    "    plt.title(k)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, '{}.pdf'.format(k)), dpi=300, format='pdf')\n",
    "    # plt.show()\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta_fr_r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
