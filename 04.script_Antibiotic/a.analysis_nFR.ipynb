{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script check nFR difference of control and exposed group at each clsuter/super-cluter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script check nFR difference of control and exposed group at each clsuter/super-cluter.\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import os\n",
    "import pandas as pd\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "import matplotlib.pyplot as plt\n",
    "import tree_util\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.multitest import fdrcorrection as fdr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute nFR for a sample\n",
    "def nfr_value(d_df, profile, sname):\n",
    "    sp_list = list(profile.columns)\n",
    "    n = len(sp_list)\n",
    "    corr = np.ones(shape=(n, n)) - d_df.loc[sp_list, sp_list].values\n",
    "    np.fill_diagonal(corr, 0)\n",
    "    # print(corr[1, 1])\n",
    "    a = np.array(profile.loc[sname, sp_list])\n",
    "    \n",
    "    inter_matrix = np.dot(a.reshape(len(a), 1),a.reshape(1, len(a)))\n",
    "    np.fill_diagonal(inter_matrix, 0)\n",
    "    td = np.sum(inter_matrix)/2\n",
    "    fr = np.sum(np.multiply(inter_matrix, corr))/2\n",
    "    fd = np.sum(np.multiply(inter_matrix, d_df.loc[sp_list, sp_list].values))/2\n",
    "    if td == 0:\n",
    "        return 0\n",
    "    return fr/td\n",
    "\n",
    "# compute nFR for all sample\n",
    "def multisample_nfr(profile, d_df, node_leaves):\n",
    "    result = pd.DataFrame(index=profile.index, columns=(list(node_leaves.keys())))\n",
    "    for sname in profile.index:\n",
    "       for node, sp_list in node_leaves.items():\n",
    "           sp_list = list(set(sp_list).intersection(set(profile.columns)))\n",
    "           selected_d = d_df.loc[sp_list, sp_list]\n",
    "           selected_profile = profile.loc[:, sp_list]\n",
    "           value = nfr_value(selected_d, selected_profile, sname)\n",
    "           result.loc[sname, node] = value\n",
    "    if 'nroot' in result.columns:\n",
    "        result.drop('nroot', axis=1, inplace=True)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '../result/Anti/nFR'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# load tree and construct tree with newick string\n",
    "with open('../result/GCN_fix_tree/renamed_GCN_tree.newick') as fp:\n",
    "    newick_tree = fp.read()\n",
    "\n",
    "json_tree = tree_util.parse(newick_tree)\n",
    "largest = {'largest': 0}\n",
    "leaf_list, l = tree_util.recu_compute(json_tree, 0, largest)\n",
    "largest_level = largest['largest']\n",
    "nlayer = largest_level\n",
    "layer_leaves_dict = tree_util.make_layer_dict(nlayer)\n",
    "tree_util.recu_layer(json_tree, layer_leaves_dict)\n",
    "tree_util.to_layer_leaves(layer_leaves_dict, nlayer)\n",
    "leaves_dict = copy.deepcopy(layer_leaves_dict)\n",
    "parent_dict = {}\n",
    "tree_util.parents(json_tree, parent_dict)\n",
    "node_leaves = {}\n",
    "for level in layer_leaves_dict.keys():\n",
    "    for node, sp_list in layer_leaves_dict[level].items():\n",
    "        if node in node_leaves.keys():\n",
    "            continue\n",
    "        node_leaves[node] = copy.deepcopy(sp_list)\n",
    "subtree_nodes = {}\n",
    "for l in leaf_list:\n",
    "    parent = parent_dict[l]\n",
    "    if parent not in subtree_nodes.keys():\n",
    "        subtree_nodes[parent] = []\n",
    "    subtree_nodes[parent].append(l)\n",
    "\n",
    "for node in node_leaves.keys():\n",
    "    parent = parent_dict[node]\n",
    "    if parent not in subtree_nodes.keys():\n",
    "        subtree_nodes[parent] = []\n",
    "    subtree_nodes[parent] += subtree_nodes[node]\n",
    "    subtree_nodes[parent].append(node)\n",
    "\n",
    "node_leaves['root'] = list(leaf_list)\n",
    "\n",
    "for node in subtree_nodes.keys():\n",
    "    subtree_nodes[node].append(node)\n",
    "\n",
    "direct_children_dict = {}\n",
    "for node, parent in parent_dict.items():\n",
    "    if parent not in direct_children_dict:\n",
    "        direct_children_dict[parent] = []\n",
    "    direct_children_dict[parent].append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load distance, abundance profile and metadata\n",
    "d_df = pd.read_csv('../data/sp_d.tsv', sep='\\t', header=0, index_col=0)\n",
    "metadata = pd.read_csv('../data/Anti/metadata.csv', sep=',', index_col=None, header=0)\n",
    "abd = pd.read_csv('../data/Anti/abd.tsv', sep='\\t', header=0, index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata[['Sample_Name', 'Timepoint', 'type']]\n",
    "metadata = metadata.drop_duplicates(inplace=False)\n",
    "sname_tmp = 'P{}E{}'\n",
    "abd_name_dict = {}\n",
    "for idx in abd.index:\n",
    "    abd_name_dict[idx] = idx.split('|')[-1].replace('_', '-')\n",
    "abd = abd.rename(index=abd_name_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = {}\n",
    "for sp in d_df.columns:\n",
    "        name_dict[sp] = sp.replace('_', '-')\n",
    "d_df = d_df.rename(columns=name_dict, index=name_dict)\n",
    "abd = abd.T/100\n",
    "sub_ids = sorted(list(set(metadata['Sample_Name'])))\n",
    "days = sorted(list(set(metadata['Timepoint'])))\n",
    "phenos = list(set(metadata['type']))\n",
    "profile = abd[list(set(abd.columns).intersection(set(d_df.columns)))]\n",
    "\n",
    "# compute nFR for all samples\n",
    "nfr_result = multisample_nfr(profile, d_df, node_leaves)\n",
    "nfr_result.to_csv(os.path.join(outdir, 'nfr_df.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_cutoff = 0.05\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "p1 = phenos[1]\n",
    "p0 = phenos[0]\n",
    "\n",
    "# differential test\n",
    "p_df = pd.DataFrame(columns=days, index=list(node_leaves.keys()))\n",
    "valid_dict = {}\n",
    "for k in node_leaves.keys():\n",
    "    metadata[k] = list(nfr_result.loc[list(metadata['Sample_Name']), k] )\n",
    "    for j in days:\n",
    "        selected0 = list(metadata[(metadata['Timepoint'] == j) & (metadata['type'] == p0)]['Sample_Name'])\n",
    "        selected1 = list(metadata[(metadata['Timepoint'] == j) & (metadata['type'] == p1)]['Sample_Name'])\n",
    "        selected_v0 = nfr_result.loc[selected0, k]\n",
    "        selected_v1 = nfr_result.loc[selected1, k]\n",
    "        if (list(selected_v0).count(0) > len(list(selected_v0))*0.8) and (list(selected_v1).count(0) > len(list(selected_v1))*0.8):\n",
    "            continue\n",
    "        if j not in valid_dict.keys():\n",
    "            valid_dict[j] = []\n",
    "        valid_dict[j].append(k)\n",
    "        t, p = mannwhitneyu(list(selected_v0), list(selected_v1))\n",
    "        p_df.loc[k, j] = p\n",
    "\n",
    "# adjust with FDR        \n",
    "p_df_adj = pd.DataFrame(columns=p_df.columns, index=list(node_leaves.keys()))\n",
    "for d in days:\n",
    "    p_values = p_df.loc[valid_dict[d], d]\n",
    "    p_adj = fdr(p_values, p_cutoff)[1]\n",
    "    for i, cluster in enumerate(valid_dict[d]):\n",
    "        p_df_adj.loc[cluster, d] = p_adj[i]\n",
    "p_df_adj.fillna(np.nan, inplace=True)\n",
    "p_df_adj.to_csv(os.path.join(outdir, 'p_value.tsv'), sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make boxplot\n",
    "for k in node_leaves.keys():\n",
    "    metadata[k] = list(nfr_result.loc[list(metadata['Sample_Name']), k] )\n",
    "    max_v = max(list(metadata[k]))\n",
    "    show = False\n",
    "    for j in days:\n",
    "        if k in valid_dict[j]:\n",
    "            show = True\n",
    "    if not show:\n",
    "        continue\n",
    "    sns.boxplot(x='Timepoint', y=k, hue='type', data=metadata)\n",
    "    for i, t in enumerate(days):\n",
    "        if pd.isna(p_df_adj.loc[k, t]):\n",
    "            plt.text(i, max_v*1.2, 'X', ha='center', va='bottom', fontsize=20, color='black')\n",
    "\n",
    "        if p_df_adj.loc[k, t] < p_cutoff:\n",
    "            plt.text(i, max_v*1.2, '**', ha='center', va='bottom', fontsize=20, color='r')\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.ylim(-0.1*max_v, max_v*1.5)\n",
    "    plt.title(k)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, '{}.pdf'.format(k)), dpi=300, format='pdf')\n",
    "    # plt.show()\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (meta_fr)",
   "language": "python",
   "name": "meta_fr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
