{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script tests difference of SE between response group and non-response group at each cluster/super-cluster and compute FR S score for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script tests difference of SE between response group and non-response group at each cluster/super-cluster and compute FR S score for each sample.\n",
    "import sys\n",
    "sys.path.append('../frstruct')\n",
    "import abd_profile\n",
    "import os\n",
    "import pandas as pd\n",
    "import GCN\n",
    "import copy\n",
    "import numpy as np\n",
    "import tree_util\n",
    "import se\n",
    "from scipy.stats import mannwhitneyu\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.multitest import fdrcorrection as fdr\n",
    "import json\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.utils import median_survival_times\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer = '../data'\n",
    "outdir = '../result/immu/SE'\n",
    "indir= os.path.join(outer, 'immu')\n",
    "# load distance, abundance profile and metadata\n",
    "# input dir: including metadata.tsv and abd.tsv\n",
    "# output result to dir\n",
    "\n",
    "ori_GCN = GCN.input_GCN('../data/gcn2008.tsv', transfer=True)\n",
    "d_df = pd.read_csv('../data/sp_d.tsv', sep='\\t', header=0, index_col=0)\n",
    "abd_path = os.path.join(indir, 'merged_species.txt')\n",
    "metadata_path = os.path.join(indir, 'metadata.txt')\n",
    "related_abd_df = pd.read_csv('../data/immu/sig.txt', sep='\\t', header=0, index_col=None)\n",
    "raw_profile = abd_profile.input_profile(abd_path, transfer=True)\n",
    "raw_profile = raw_profile[list(set(d_df.index).intersection(set(raw_profile.columns)))]\n",
    "metadata = pd.read_csv(metadata_path, sep=',', header=0)\n",
    "metadata.fillna('NA', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tree and construct tree with newick string\n",
    "with open('../result/GCN_fix_tree/renamed_GCN_tree.newick') as fp:\n",
    "    newick_tree = fp.read()\n",
    "json_tree = tree_util.parse(newick_tree)\n",
    "largest = {'largest': 0}\n",
    "leaf_list, l = tree_util.recu_compute(json_tree, 0, largest)\n",
    "largest_level = largest['largest']\n",
    "nlayer = largest_level\n",
    "leaf_list, l = tree_util.recu_compute(json_tree, 0, largest)\n",
    "layer_leaves_dict = tree_util.make_layer_dict(nlayer)\n",
    "\n",
    "tree_util.recu_layer(json_tree, layer_leaves_dict)\n",
    "tree_util.to_layer_leaves(layer_leaves_dict, nlayer)\n",
    "name_dict = {}\n",
    "for sp in d_df.columns:\n",
    "        name_dict[sp] = sp.replace('_', '-')\n",
    "renamed_GCN = ori_GCN[list(d_df.columns)].rename(columns=name_dict)\n",
    "renamed_d = d_df.rename(columns=name_dict, index=name_dict)\n",
    "result = {}\n",
    "# compute leaf layer\n",
    "result['leaves_dict'] = copy.deepcopy(layer_leaves_dict)\n",
    "parent_dict = {}\n",
    "tree_util.parents(json_tree, parent_dict)\n",
    "node_leaves = {}\n",
    "for level in layer_leaves_dict.keys():\n",
    "    for node, sp_list in layer_leaves_dict[level].items():\n",
    "        if node in node_leaves.keys():\n",
    "            continue\n",
    "        node_leaves[node] = copy.deepcopy(sp_list)\n",
    "subtree_nodes = {}\n",
    "for l in leaf_list:\n",
    "    parent = parent_dict[l]\n",
    "    if parent not in subtree_nodes.keys():\n",
    "        subtree_nodes[parent] = []\n",
    "    subtree_nodes[parent].append(l)\n",
    "\n",
    "for node in node_leaves.keys():\n",
    "    parent = parent_dict[node]\n",
    "    if parent not in subtree_nodes.keys():\n",
    "        subtree_nodes[parent] = []\n",
    "    subtree_nodes[parent] += subtree_nodes[node]\n",
    "    subtree_nodes[parent].append(node)\n",
    "\n",
    "for node in subtree_nodes.keys():\n",
    "    subtree_nodes[node].append(node)\n",
    "\n",
    "direct_children_dict = {}\n",
    "for node, parent in parent_dict.items():\n",
    "    if parent not in direct_children_dict:\n",
    "        direct_children_dict[parent] = []\n",
    "    direct_children_dict[parent].append(node)\n",
    "\n",
    "result['leaves_dict']['root'] = {'root': list(name_dict.values())}\n",
    "node_leaves['root'] = list(name_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute FR network\n",
    "def fr(d_df, profile, sname):\n",
    "    #print(profile.columns, d_df.index)\n",
    "    sp_list = list(set(profile.columns).intersection(set(d_df.index)))\n",
    "    #print(sp_list)\n",
    "    sp_d_df = d_df.loc[sp_list, sp_list]\n",
    "    #print(profile)\n",
    "    sp_profile = np.array(profile.loc[sname, sp_list])\n",
    "    value = np.dot(sp_profile.reshape(len(sp_profile), 1),sp_profile.reshape(1, len(sp_profile)))\n",
    "    width = value.shape[0]\n",
    "    cor_df = np.ones(shape=(width, width)) - sp_d_df.values\n",
    "    for i in range(width):\n",
    "        cor_df[i][i] = 0\n",
    "    value = np.multiply(value, cor_df)\n",
    "    fr_df = pd.DataFrame(value, index=sp_list, columns=sp_list)\n",
    "    return fr_df\n",
    "\n",
    "# compute SE for all sample\n",
    "def multisample_se(profile, d_df, parent_dict, node_leaves, child_dict, direct_children_dict, param):\n",
    "    result = pd.DataFrame(index=profile.index, columns=(list(node_leaves.keys())))\n",
    "    for sname in profile.index:\n",
    "        # print(sname)\n",
    "        edge_df = fr(d_df, profile, sname)\n",
    "        tmp = se.subtree_se_adj(edge_df, parent_dict, node_leaves, child_dict, direct_children_dict, param)\n",
    "        for node in tmp.keys():\n",
    "            value = tmp[node]\n",
    "            result.loc[sname, node] = value\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_list = ['Disc', 'Valid']\n",
    "cohort_se_dict = {}\n",
    "param = 0.5\n",
    "# compute SE for all samples\n",
    "for cohort in cohort_list:\n",
    "    metadata_cohort = metadata[(metadata['Cohort'] == cohort)] #  \n",
    "    metadata_cohort = metadata_cohort[metadata_cohort['Sample Name'].isin(list(raw_profile.index))]\n",
    "\n",
    "\n",
    "    selected_raw_profile = raw_profile.loc[list(metadata_cohort['Sample Name']), :]\n",
    "    crc_profile = abd_profile.check(selected_raw_profile, d_df)\n",
    "\n",
    "    pheno_list = {}\n",
    "    pheno_profiles = {}\n",
    "    for i in metadata_cohort.index:\n",
    "        pheno = metadata_cohort.loc[i, 'OS12']\n",
    "        if pheno == 'NA':\n",
    "            continue\n",
    "        if pheno not in pheno_list.keys():\n",
    "            pheno_list[pheno] = []\n",
    "        if metadata_cohort.loc[i, 'Sample Name'] in list(crc_profile.index):\n",
    "            pheno_list[pheno].append(metadata_cohort.loc[i, 'Sample Name'])\n",
    "\n",
    "    for c, clist in pheno_list.items():\n",
    "        pheno_profiles[c] = copy.deepcopy(crc_profile.loc[clist, :])\n",
    "    \n",
    "    se_result = {}   \n",
    "    for p, profile in pheno_profiles.items():\n",
    "        renamed_p = profile.rename(columns=name_dict)\n",
    "        p_result = multisample_se(renamed_p, renamed_d, parent_dict, node_leaves, subtree_nodes, direct_children_dict, param)\n",
    "        se_result[p] = p_result\n",
    "\n",
    "    cohort_se_dict[cohort] = copy.deepcopy(se_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output se to files\n",
    "for cohort in cohort_se_dict.keys():\n",
    "    odir = os.path.join(outdir, cohort)\n",
    "    if not os.path.exists(odir):\n",
    "        os.makedirs(odir)\n",
    "    \n",
    "    phenos = list(cohort_se_dict[cohort].keys())\n",
    "    for pheno in phenos:\n",
    "        cohort_se_dict[cohort][pheno].to_csv(os.path.join(odir, 'se_{}.tsv'.format(pheno)), sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "p_cutoff = 0.05\n",
    "\n",
    "# differential test \n",
    "p_df = pd.DataFrame()\n",
    "mean_df = pd.DataFrame()\n",
    "valid_dict = {}\n",
    "show_node = []\n",
    "\n",
    "p_detail = pd.DataFrame(columns=['p', 'mean(R)', 'mean(NR)', 'var(R)', 'var(NR)', 'fc(R/NR)', 'diff(R-NR)', 'abs', 'non_zero(NR)', 'non_zero(R)', 'non_zero%(NR)', 'non_zero%(R)']) \n",
    "for cohort in cohort_se_dict.keys():\n",
    "    \n",
    "    if 'NR' not in cohort_se_dict[cohort].keys():\n",
    "        continue\n",
    "    if 'R' not in cohort_se_dict[cohort].keys():\n",
    "        continue\n",
    "    if cohort not in valid_dict.keys():\n",
    "        valid_dict[cohort] = []\n",
    "    l_se1 = cohort_se_dict[cohort]['NR']\n",
    "    l_se2 = cohort_se_dict[cohort]['R']\n",
    "    for inode in l_se1.columns:\n",
    "        if (list(l_se1[inode]).count(0) > len(list(l_se1[inode]))*0.8) and (list(l_se2[inode]).count(0) > len(list(l_se2[inode]))*0.8):\n",
    "            continue\n",
    "        valid_dict[cohort].append(inode)\n",
    "        R_v = list(l_se2[inode])\n",
    "        NR_v = list(l_se1[inode])\n",
    "        \n",
    "        t, p1 = mannwhitneyu(R_v, NR_v)\n",
    "        if cohort == 'Disc':\n",
    "            p_detail.loc[inode, 'mean(R)'] = np.mean(R_v)\n",
    "            p_detail.loc[inode, 'mean(NR)'] = np.mean(NR_v)\n",
    "            p_detail.loc[inode, 'var(R)'] = np.var(R_v)\n",
    "            p_detail.loc[inode, 'var(NR)'] = np.var(NR_v)\n",
    "            p_detail.loc[inode, 'diff(R-NR)'] = p_detail.loc[inode, 'mean(R)'] - p_detail.loc[inode, 'mean(NR)']\n",
    "            p_detail.loc[inode, 'fc(R/NR)'] = p_detail.loc[inode, 'mean(R)']/p_detail.loc[inode, 'mean(NR)']\n",
    "            p_detail.loc[inode, 'non_zero(R)'] = len(R_v) - R_v.count(0)\n",
    "            p_detail.loc[inode, 'non_zero(NR)'] = len(NR_v) - NR_v.count(0)\n",
    "            p_detail.loc[inode, 'non_zero%(R)'] = p_detail.loc[inode, 'non_zero(R)']/len(R_v)\n",
    "            p_detail.loc[inode, 'non_zero%(NR)'] = p_detail.loc[inode, 'non_zero(NR)']/len(NR_v)\n",
    "            p_detail.loc[inode, 'p'] = p1\n",
    "        p_df.loc[cohort, inode] = p1\n",
    "        mean_df.loc[cohort, inode] = np.mean(l_se1[inode]) - np.mean(l_se2[inode])\n",
    "p_df.fillna(np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust p-values by FDR\n",
    "for dir in p_df.index:\n",
    "    valid_nodes = valid_dict[dir]\n",
    "    p_vector = list(p_df.loc[dir, valid_nodes])\n",
    "    p_adj = fdr(p_vector, p_cutoff)[1]\n",
    "    for i, node in enumerate(valid_nodes):\n",
    "        p_df.loc[dir, node] = p_adj[i]\n",
    "        if dir == 'Disc':\n",
    "            p_detail.loc[node, 'p_adj'] = p_adj[i]\n",
    "            p_detail.loc[node, 'abs'] = abs(p_detail.loc[node, 'diff(R-NR)'])\n",
    "p_detail.to_csv(os.path.join(outdir, 'p_detail.tsv'), sep='\\t')\n",
    "p_df.to_csv(os.path.join(outdir, 'p_all_cohorts.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_plot = pd.DataFrame()\n",
    "for dir in p_df.index:\n",
    "    for inode in p_df.columns:\n",
    "        p1 = p_df.loc[dir, inode]\n",
    "        if pd.isna(p1):\n",
    "            node_plot.loc[dir, inode] = np.nan\n",
    "        elif p1 < p_cutoff:\n",
    "            if mean_df.loc[dir, inode] > 0:\n",
    "                node_plot.loc[dir, inode] = 1\n",
    "            else:\n",
    "                node_plot.loc[dir, inode] = -1\n",
    "        else:\n",
    "            node_plot.loc[dir, inode] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "show_node = []   \n",
    "part_df = node_plot\n",
    "for inode in part_df.columns:\n",
    "    all_zero = True\n",
    "    all_nan = True\n",
    "    for dir in part_df.index:\n",
    "        if not pd.isna(node_plot.loc[dir, inode]):\n",
    "            all_nan = False\n",
    "        if node_plot.loc[dir, inode] != 0:\n",
    "            all_zero = False\n",
    "    if not (all_zero or all_nan):\n",
    "        show_node.append(inode)\n",
    "plt.figure(figsize=(5, 12))\n",
    "part_df = part_df[show_node]\n",
    "non_zero_count = part_df.astype(bool).sum(axis=0) - part_df.isnull().sum(axis=0)\n",
    "sorted_df = part_df.iloc[:, non_zero_count.argsort()]\n",
    "#show_node = sorted(show_node)\n",
    "sns.heatmap(sorted_df.T, vmax=1, vmin=-1, square=True, linecolor='black', cbar=False, xticklabels=True, yticklabels=True, linewidths=1, cmap=\"coolwarm\")\n",
    "plt.title('Different SE of NSCLC cohorts')\n",
    "plt.xticks(rotation=90)\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.spines['top'].set_visible(True)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "ax.spines['right'].set_visible(True)\n",
    "opath = os.path.join(outdir, 'NSCLC.pdf')\n",
    "plt.tight_layout()\n",
    "plt.savefig(opath, dpi=300, format='pdf')\n",
    "part_df[show_node].to_csv(os.path.join(outdir, 'NSCLC.tsv'), sep='\\t')\n",
    "plt.clf()\n",
    "#plt.show()\n",
    "#print(part_df[show_node].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "abd_path = os.path.join(indir, 'merged_species.txt')\n",
    "raw_profile = abd_profile.input_profile(abd_path, transfer=True)\n",
    "raw_profile = raw_profile[list(set(d_df.index).intersection(set(raw_profile.columns)))]\n",
    "metadata_path = os.path.join(indir, 'metadata.txt')\n",
    "metadata = pd.read_csv(metadata_path, sep=',', header=0)\n",
    "metadata.fillna('NA', inplace=True)\n",
    "metadata_cohort = metadata[(metadata['Cohort'] == 'Disc')] #  \n",
    "metadata_cohort = metadata_cohort[metadata_cohort['Sample Name'].isin(list(raw_profile.index))]\n",
    "selected_raw_profile = raw_profile.loc[list(metadata_cohort['Sample Name']), :]\n",
    "disc_profile = abd_profile.check(selected_raw_profile, d_df)\n",
    "delete_sp = []\n",
    "binary_disc = copy.deepcopy(disc_profile)\n",
    "binary_disc[binary_disc > 0] = 1\n",
    "for sp in disc_profile.columns:\n",
    "    if binary_disc[sp].sum() < len(disc_profile)*2.5/100:\n",
    "        delete_sp.append(sp)\n",
    "disc_profile.drop(columns=delete_sp, inplace=True)\n",
    "clusters = ['supercluster_S3', 'cluster_S1-C20']\n",
    "interested_dict = {}\n",
    "num_dict = {}\n",
    "cluster_sp_dict = {}\n",
    "for cluster in clusters:\n",
    "    leaves = node_leaves[cluster]\n",
    "    leaves = [leaf.replace('-', '_') for leaf in leaves]\n",
    "    common_sp = list(set(disc_profile.columns).intersection(set(leaves)))\n",
    "    interested_dict[cluster] = copy.deepcopy(disc_profile[common_sp])\n",
    "    num_dict[cluster] = len(common_sp)\n",
    "    cluster_sp_dict[cluster] = ','.join(common_sp)\n",
    "\n",
    "with open(os.path.join(outdir, 'cluster.tsv'), 'w') as fp:\n",
    "    s = 'cluster\\tsize\\tleaves\\n'\n",
    "    for cluster in clusters:\n",
    "        s += '{}\\t{}\\t{}\\n'.format(cluster, num_dict[cluster], ','.join(list(interested_dict[cluster].columns)))\n",
    "    fp.write(s)\n",
    "\n",
    "json.dump(cluster_sp_dict, open(os.path.join(outdir, 'cluster_sp.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute FR S Score\n",
    "existed_sp = {}\n",
    "cluster_ratio = {}\n",
    "for cluster in clusters:\n",
    "    cluster_p = interested_dict[cluster]\n",
    "    column_medians = cluster_p.median()\n",
    "    for sp in cluster_p.columns:\n",
    "        cluster_p.loc[:, sp] = cluster_p.loc[:, sp] - column_medians[sp]\n",
    "    cluster_p[cluster_p > 0] = 1\n",
    "    cluster_p[cluster_p < 0] = 0\n",
    "    total_n = num_dict[cluster]\n",
    "    # sum by row\n",
    "    sum_result = cluster_p.sum(axis=1)\n",
    "    ratio = sum_result/total_n\n",
    "    cluster_ratio[cluster] = copy.deepcopy(ratio)\n",
    "    existed_sp[cluster] = {}\n",
    "    for sample in cluster_p.index:\n",
    "        existed_sp[cluster][sample] = []\n",
    "        for sp in cluster_p.columns:\n",
    "            if cluster_p.loc[sample, sp] == 1:\n",
    "                existed_sp[cluster][sample].append(sp)\n",
    "        existed_sp[cluster][sample] = ','.join(existed_sp[cluster][sample])\n",
    "json.dump(existed_sp, open(os.path.join(outdir, 'existed_sp.json'), 'w'))\n",
    "result_score = (cluster_ratio[clusters[1]]-cluster_ratio[clusters[0]][cluster_ratio[clusters[0]].index]+1)/2\n",
    "result_score.to_csv(os.path.join(outdir, 'score_disc.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2765957446808511\n",
      "0.6222222222222222\n",
      "SIG2:       SIG2_lower_0.95  SIG2_upper_0.95\n",
      "0.5             12.6        24.194609\n",
      "SIG1:       SIG1_lower_0.95  SIG1_upper_0.95\n",
      "0.5              2.5         8.547009\n",
      "0.723404255319149 34 47\n",
      "0.37777777777777777 34 90\n"
     ]
    }
   ],
   "source": [
    "# merge FR S score to metadata and binary classification TOPOB01 0 for NR, 1 for R\n",
    "DS1 = pd.read_csv('../data/immu/DS1_oncology_clinical_data.csv', header=0)\n",
    "DS1 = DS1[DS1['Cohort']=='Disc']\n",
    "DS1.set_index('Sample_id', inplace=True)\n",
    "DS1.dropna(subset=['OS12'], inplace=True)\n",
    "DS1['Sample_id'] = DS1.index\n",
    "\n",
    "pred_disc = pd.read_csv(os.path.join(outdir, 'score_disc.tsv'), sep='\\t', header=0, index_col=0)\n",
    "DS1['TOPOB01'] = pred_disc.loc[DS1.index, '0']\n",
    "DS1.to_csv(os.path.join(outdir, 'pred_disc.tsv'), sep='\\t', index=False)\n",
    "pred_DS1 = DS1[(DS1['TOPOB01'] <= 0.498) | (DS1['TOPOB01'] > 0.629)]\n",
    "pred_DS1.to_csv(os.path.join(outdir, 'pred_valid.tsv'), sep='\\t', index=False)\n",
    "DS1.loc[DS1[DS1['TOPOB01'] <= 0.498].index, 'TOPOB01'] = 0\n",
    "DS1.loc[DS1[DS1['TOPOB01'] > 0.629].index, 'TOPOB01'] = 1\n",
    "pred_DS1 = DS1[(DS1['TOPOB01'] == 0) | (DS1['TOPOB01'] == 1)]\n",
    "pred_DS1.to_csv(os.path.join(outdir, 'pred_binary_disc.tsv'), sep='\\t', index=False)\n",
    "print(len(pred_DS1[((pred_DS1['OS12']=='R')&(pred_DS1['TOPOB01'] == 0))])/len(pred_DS1[pred_DS1['TOPOB01'] == 0]))\n",
    "print(len(pred_DS1[((pred_DS1['OS12']=='R')&(pred_DS1['TOPOB01'] == 1))])/len(pred_DS1[pred_DS1['TOPOB01'] == 1]))\n",
    "# NR [47] < 0.498 < gray_zone [93] < 0.629 < R [90]\n",
    "# auc = 0.66\n",
    "# HR = 0.59 [0.43-0.80], p = 8.6e-04\n",
    "\n",
    "# Plot os curve\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(pred_DS1[pred_DS1['TOPOB01']==1]['OS'], event_observed=pred_DS1[pred_DS1['TOPOB01']==1]['Death'], label='SIG2')\n",
    "ax = kmf.plot(figsize=(4.2,4))\n",
    "treatment_median_confidence_interval_ = median_survival_times(kmf.confidence_interval_)\n",
    "print('SIG2: ', treatment_median_confidence_interval_)\n",
    "\n",
    "kmf.fit(pred_DS1[pred_DS1['TOPOB01']==0]['OS'], event_observed=pred_DS1[pred_DS1['TOPOB01']==0]['Death'], label = 'SIG1')\n",
    "ax = kmf.plot(ax=ax)\n",
    "\n",
    "control_median_confidence_interval_ = median_survival_times(kmf.confidence_interval_)\n",
    "print('SIG1: ', control_median_confidence_interval_)\n",
    "\n",
    "plt.ylabel('Overall Survival')\n",
    "plt.xlabel('Time (Months)')\n",
    "plt.title('SE S Score')\n",
    "plt.savefig(os.path.join(outdir, 'OS_curve.pdf'), dpi=300, format='pdf')\n",
    "\n",
    "# add akk information and binary classification TOPOB01\n",
    "for sample in DS1.index:\n",
    "    if DS1.loc[sample, 'TOPOB01'] < 1 and DS1.loc[sample, 'TOPOB01'] > 0:\n",
    "        if DS1.loc[sample, 'AKK_TRICHO'] == 'Low':\n",
    "            DS1.loc[sample, 'TOPOB01'] = 1\n",
    "        else:\n",
    "            DS1.loc[sample, 'TOPOB01'] = 0\n",
    "DS1.to_csv(os.path.join(outdir, 'pred_disc_akk.tsv'), sep='\\t', index=False)\n",
    "\n",
    "print(len(pred_DS1[((pred_DS1['OS12']=='NR')&(pred_DS1['TOPOB01'] == 0))])/len(pred_DS1[pred_DS1['TOPOB01'] == 0]), len(pred_DS1[((pred_DS1['OS12']=='NR')&(pred_DS1['TOPOB01'] == 0))]), len(pred_DS1[pred_DS1['TOPOB01'] == 0]))\n",
    "print(len(pred_DS1[((pred_DS1['OS12']=='NR')&(pred_DS1['TOPOB01'] == 1))])/len(pred_DS1[pred_DS1['TOPOB01'] == 1]), len(pred_DS1[((pred_DS1['OS12']=='NR')&(pred_DS1['TOPOB01'] == 1))]), len(pred_DS1[pred_DS1['TOPOB01'] == 1]))\n",
    "\n",
    "# auc = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supercluster_S3 11\n",
      "cluster_S1-C20 19\n"
     ]
    }
   ],
   "source": [
    "for sig, slist in cluster_sp_dict.items():\n",
    "    print(sig, len(slist.split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5981308411214953\n",
      "0.3902439024390244\n",
      "107\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "# add akk abundance profile and binary classification TOPOB01\n",
    "DS1 = pd.read_csv('../data/immu/DS1_oncology_clinical_data.csv', header=0)\n",
    "DS1 = DS1[DS1['Cohort']=='Disc']\n",
    "DS1.set_index('Sample_id', inplace=True)\n",
    "DS1.dropna(subset=['OS12'], inplace=True)\n",
    "DS1['Sample_id'] = DS1.index\n",
    "\n",
    "pred_disc = pd.read_csv(os.path.join(outdir, 'score_disc.tsv'), sep='\\t', header=0, index_col=0)\n",
    "DS1['TOPOB01'] = pred_disc.loc[DS1.index, '0']\n",
    "DS1.to_csv(os.path.join(outdir, 'pred_disc.tsv'), sep='\\t', index=False)\n",
    "\n",
    "DS1.loc[DS1[DS1['TOPOB01'] < 0.498].index, 'TOPOB01'] = 0\n",
    "DS1.loc[DS1[DS1['TOPOB01'] >= 0.629].index, 'TOPOB01'] = 1\n",
    "pred_DS1 = DS1[(DS1['TOPOB01'] == 0) | (DS1['TOPOB01'] == 1)]\n",
    "pred_DS1.to_csv(os.path.join(outdir, 'pred_binary_disc.tsv'), sep='\\t', index=False)\n",
    "\n",
    "# add akk\n",
    "for sample in DS1.index:\n",
    "    akk_abd = disc_profile.loc[sample, 's__Akkermansia_muciniphila']\n",
    "    if akk_abd > 0.04799:\n",
    "        DS1.loc[sample, 'AKK_TRICHO'] = 'High'\n",
    "    elif akk_abd <= 0.00035:\n",
    "        DS1.loc[sample, 'AKK_TRICHO'] = 'Zero'\n",
    "    else:\n",
    "        DS1.loc[sample, 'AKK_TRICHO'] = 'Low'\n",
    "        \n",
    "    if DS1.loc[sample, 'TOPOB01'] < 1 and DS1.loc[sample, 'TOPOB01'] > 0:\n",
    "        if DS1.loc[sample, 'AKK_TRICHO'] == 'Low':\n",
    "            DS1.loc[sample, 'TOPOB01'] = 1\n",
    "        else:\n",
    "            DS1.loc[sample, 'TOPOB01'] = 0\n",
    "DS1.to_csv(os.path.join(outdir, 'pred_disc_akk4.tsv'), sep='\\t', index=False)\n",
    "\n",
    "print(len(DS1[((DS1['OS12']=='NR')&(DS1['TOPOB01'] == 0))])/len(DS1[DS1['TOPOB01'] == 0]))\n",
    "print(len(DS1[((DS1['OS12']=='NR')&(DS1['TOPOB01'] == 1))])/len(DS1[DS1['TOPOB01'] == 1]))\n",
    "print(len(DS1[DS1['TOPOB01'] == 0]))\n",
    "print(len(DS1[DS1['TOPOB01'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta_fr_r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
