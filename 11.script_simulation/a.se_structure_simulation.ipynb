{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd \n",
    "import random\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import fdrcorrection as fdr\n",
    "sys.path.append('../src')\n",
    "import abd_profile\n",
    "import numpy as np\n",
    "import se\n",
    "import tree_util\n",
    "import copy\n",
    "from scipy import stats\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fr(d_df, profile, sname):\n",
    "    #print(profile.columns, d_df.index)\n",
    "    sp_list = sorted(list(set(profile.columns).intersection(set(d_df.index))))\n",
    "    #print(sp_list)\n",
    "    sp_d_df = d_df.loc[sp_list, sp_list]\n",
    "    #print(profile)\n",
    "    sp_profile = np.array(profile.loc[sname, sp_list])\n",
    "    value = np.dot(sp_profile.reshape(len(sp_profile), 1),sp_profile.reshape(1, len(sp_profile)))\n",
    "    width = value.shape[0]\n",
    "    cor_df = np.ones(shape=(width, width)) - sp_d_df.values\n",
    "    for i in range(width):\n",
    "        cor_df[i][i] = 0\n",
    "    value = np.multiply(value, cor_df)\n",
    "    fr_df = pd.DataFrame(value, index=sp_list, columns=sp_list)\n",
    "    return fr_df\n",
    "\n",
    "def select_edges_assign(df, pairs, remaining_pairs, nsample=200):\n",
    "    triu_indices = np.triu_indices_from(df, k=1)\n",
    "    upper_triangle_values = df.values[triu_indices]\n",
    "    #print(upper_triangle_values)\n",
    "    #samples = random.sample(upper_triangle_values.value(), nsample)\n",
    "    samples = sorted(list(upper_triangle_values), reverse=True)[:nsample]\n",
    "    \n",
    "    # random put into the new_df\n",
    "    new_df = pd.DataFrame(index=df.index, columns=df.columns, data=0)\n",
    "    #upper_triangle_pairs = [(df.index[i], df.columns[j]) for i, j in zip(*triu_indices)]\n",
    "    upper_triangle_pairs = pairs + remaining_pairs\n",
    "    sample_pairs = random.sample(upper_triangle_pairs, nsample)\n",
    "    for idx, (i, j) in enumerate(sample_pairs):\n",
    "        new_df.loc[i, j] = samples[idx]\n",
    "    rand_df = copy.deepcopy(new_df+new_df.T)\n",
    "    \n",
    "   \n",
    "    # random put into the pairs\n",
    "    new_df = pd.DataFrame(index=df.index, columns=df.columns, data=0)\n",
    "    sample_pairs = random.sample(pairs, nsample)\n",
    "    for idx, (i, j) in enumerate(sample_pairs):\n",
    "        new_df.loc[i, j] = samples[idx]\n",
    "    in_df = copy.deepcopy(new_df+new_df.T)\n",
    "\n",
    "    # random put outside the pairs\n",
    "    new_df = pd.DataFrame(index=df.index, columns=df.columns, data=0)\n",
    "    sample_pairs = random.sample(remaining_pairs, nsample)\n",
    "    for idx, (i, j) in enumerate(sample_pairs):\n",
    "        new_df.loc[i, j] = samples[idx]\n",
    "    out_df = copy.deepcopy(new_df+new_df.T)\n",
    "    return rand_df, in_df, out_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n: repeat time\n",
    "# k: sample number\n",
    "\n",
    "def summary(se_list_rand, se_list_high, se_list_low):\n",
    "    \n",
    "    # run paired Wilcoxon test\n",
    "    statistic, p1 = stats.wilcoxon(se_list_high, se_list_rand)\n",
    "    statistic, p2 = stats.wilcoxon(se_list_low, se_list_rand)\n",
    "    statistic, p3 = stats.wilcoxon(se_list_high, se_list_low)\n",
    "    print(\"rand mean: \", np.mean(se_list_rand), 'rand std: ', np.std(se_list_rand))\n",
    "    print(\"high mean: \", np.mean(se_list_high), 'high std: ', np.std(se_list_high))\n",
    "    print(\"low mean: \", np.mean(se_list_low), 'low std: ', np.std(se_list_low))\n",
    "    print(\"rand vs high: \", p1)\n",
    "    print(\"rand vs low: \", p2)\n",
    "    print(\"high vs low: \", p3)\n",
    "\n",
    "def get_ori_nets(profile, d_df):\n",
    "    net_dict = {}\n",
    "    rename_dict = {}\n",
    "    for sp in d_df.index:\n",
    "        rename_dict[sp] = sp.replace('_', '-')\n",
    "    renamed_d = d_df.rename(columns=rename_dict, index=rename_dict)\n",
    "\n",
    "    renamed_profile = profile.rename(columns=rename_dict)\n",
    "    renamed_profile = renamed_profile[list(set(renamed_profile.columns).intersection(set(renamed_d.index)))]\n",
    "    for sname in profile.index:\n",
    "        # print(sname)\n",
    "        edge_df = fr(renamed_d, renamed_profile, sname)\n",
    "        net_dict[sname] = copy.deepcopy(edge_df)\n",
    "        # tmp = se.subtree_se_adj(edge_df, parent_dict, node_leaves, subtree_nodes, direct_children_dict, param)\n",
    "        # for node in tmp.keys():\n",
    "        #     value = tmp[node]\n",
    "        #     result.loc[sname, node] = value\n",
    "    return net_dict\n",
    "\n",
    "def split_newick(newick_tree):\n",
    "    json_tree = tree_util.parse(newick_tree)\n",
    "    largest = {'largest': 0}\n",
    "    leaf_list, l = tree_util.recu_compute(json_tree, 0, largest)\n",
    "    largest_level = largest['largest']\n",
    "    nlayer = largest_level\n",
    "    leaf_list, l = tree_util.recu_compute(json_tree, 0, largest)\n",
    "    layer_leaves_dict = tree_util.make_layer_dict(nlayer)\n",
    "\n",
    "    tree_util.recu_layer(json_tree, layer_leaves_dict)\n",
    "    tree_util.to_layer_leaves(layer_leaves_dict, nlayer)\n",
    "    result = {}\n",
    "    # compute leaf layer\n",
    "    result['leaves_dict'] = copy.deepcopy(layer_leaves_dict)\n",
    "    parent_dict = {}\n",
    "    tree_util.parents(json_tree, parent_dict)\n",
    "    node_leaves = {}\n",
    "    for level in layer_leaves_dict.keys():\n",
    "        for node, sp_list in layer_leaves_dict[level].items():\n",
    "            if node in node_leaves.keys():\n",
    "                continue\n",
    "            node_leaves[node] = copy.deepcopy(sp_list)\n",
    "    subtree_nodes = {}\n",
    "    for l in leaf_list:\n",
    "        parent = parent_dict[l]\n",
    "        if parent not in subtree_nodes.keys():\n",
    "            subtree_nodes[parent] = []\n",
    "        subtree_nodes[parent].append(l)\n",
    "\n",
    "    for node in node_leaves.keys():\n",
    "        parent = parent_dict[node]\n",
    "        if parent not in subtree_nodes.keys():\n",
    "            subtree_nodes[parent] = []\n",
    "        subtree_nodes[parent] += subtree_nodes[node]\n",
    "        subtree_nodes[parent].append(node)\n",
    "\n",
    "    for node in subtree_nodes.keys():\n",
    "        subtree_nodes[node].append(node)\n",
    "\n",
    "    direct_children_dict = {}\n",
    "    for node, parent in parent_dict.items():\n",
    "        if parent not in direct_children_dict:\n",
    "            direct_children_dict[parent] = []\n",
    "        direct_children_dict[parent].append(node)\n",
    "    return parent_dict, node_leaves, subtree_nodes, direct_children_dict, leaf_list\n",
    "\n",
    "def get_se_list(net_dict, parent_dict, node_leaves, subtree_nodes, direct_children_dict, param=0.5):\n",
    "    result = pd.DataFrame()\n",
    "    for sname in net_dict.keys():\n",
    "        edge_df = net_dict[sname]\n",
    "        # for i, edge_df in enumerate(net_dict[sname]):\n",
    "        #     nid = \"{}_{}\".format(sname, i)\n",
    "        nid = sname\n",
    "        tmp = se.subtree_se_adj(edge_df, parent_dict, node_leaves, subtree_nodes, direct_children_dict, param)\n",
    "        for node in tmp.keys():\n",
    "            value = tmp[node]\n",
    "            result.loc[nid, node] = value\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_cohort(indir):\n",
    "    # Load the dataset\n",
    "    abd_path = os.path.join(indir, 'abd.tsv')\n",
    "    metadata_path = os.path.join(indir, 'metadata.tsv')\n",
    "    # first process data from gutmeta\n",
    "    raw_profile = abd_profile.input_profile(abd_path, transfer=True)\n",
    "    metadata = pd.read_csv(metadata_path, sep='\\t', header=0)\n",
    "    selected_raw_profile = raw_profile.loc[list(metadata['sample_id']), :]\n",
    "    selected_raw_profile = abd_profile.rename_s_level(selected_raw_profile)\n",
    "    crc_profile = abd_profile.clean(selected_raw_profile)\n",
    "\n",
    "    pheno_list = {}\n",
    "    for i in range(metadata.shape[0]):\n",
    "        pheno = metadata.loc[i, 'disease']\n",
    "        if pheno not in pheno_list.keys():\n",
    "            pheno_list[pheno] = []\n",
    "        if metadata.loc[i, 'sample_id'] in list(crc_profile.index):\n",
    "            pheno_list[pheno].append(metadata.loc[i, 'sample_id'])\n",
    "\n",
    "    # se_dict = {}\n",
    "    # for c in pheno_list.keys():\n",
    "    #     se_dict[c] = pd.read_csv(os.path.join(sedir, 'se_{}.tsv'.format(c)), sep='\\t', header=0, index_col=0)\n",
    "\n",
    "    # se_df = pd.DataFrame()\n",
    "    # for c in pheno_list.keys():\n",
    "    #     se_df = pd.concat([se_df, se_dict[c]], axis=0)\n",
    "\n",
    "    # #slist = pheno_list['Health']\n",
    "    return crc_profile\n",
    "    # return crc_profile[sorted(crc_profile.columns)], se_df.loc[slist, ]\n",
    "\n",
    "def sampling(total, cluster_list, slist):\n",
    "    clusters = random.choices(cluster_list, k=total)\n",
    "    samples = random.choices(slist, k=total)\n",
    "    return samples, clusters\n",
    "\n",
    "def sampling_s(total, slist):\n",
    "    samples = random.choices(slist, k=total)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_dir = '../data'\n",
    "total_time = 10\n",
    "outer_se = '../result/large_scale_cohort'\n",
    "output_dir = '../result/validation/se_structure_simulation'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "plist = ['CRC']\n",
    "d_df = pd.read_csv('../data/sp_d.tsv', sep='\\t', header=0, index_col=0)\n",
    "diff_df = pd.read_csv(os.path.join(outer_se, 'p_all_cohorts_se.tsv'), sep='\\t', header=0, index_col=0)\n",
    "with open('../result/GCN_fix_tree/renamed_GCN_tree.newick') as fp:\n",
    "    newick_tree = fp.read()\n",
    "    parent_dict, node_leaves, subtree_nodes, direct_children_dict, leaf_list = split_newick(newick_tree)\n",
    "pheno_result= {}\n",
    "ori_se_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CRC ==\n",
      "-- CRC -- CRC1 --\n",
      "-- CRC -- CRC2 --\n",
      "-- CRC -- CRC3 --\n",
      "-- CRC -- CRC4 --\n",
      "-- CRC -- CRC5 --\n",
      "-- CRC -- CRC6 --\n",
      "-- CRC -- CRC7 --\n",
      "-- CRC -- CRC8 --\n",
      "-- CRC -- CRC9 --\n",
      "pairs:  191890\n",
      "remaining pairs:  189611\n"
     ]
    }
   ],
   "source": [
    "cluster = 'supercluster_S1'\n",
    "nsample = 189000\n",
    "for pheno in plist:\n",
    "    result = pd.DataFrame(columns = ['sample', 'cluster', 'rand', 'high', 'low'])\n",
    "    data_dir = os.path.join(outer_dir, pheno)\n",
    "    se_dir = os.path.join(outer_se, pheno)\n",
    "    print('== {} =='.format(pheno))\n",
    "    merged_crc_profile = pd.DataFrame()\n",
    "    for cohort in os.listdir(data_dir):\n",
    "        print(f'-- {pheno} -- {cohort} --')\n",
    "        # sp_diff_df = pd.read_csv(os.path.join(abd_dff_dir, pheno, 'p_{}.tsv'.format(cohort)), sep='\\t', header=0, index_col=0)\n",
    "        se_cohort_dir = os.path.join(se_dir, cohort)\n",
    "        #ori_se = pd.read_csv(os.path.join(se_cohort_dir, 'se_Health.tsv'), sep='\\t', header=0, index_col=0)\n",
    "        abd_dir = os.path.join(data_dir, cohort)\n",
    "        #crc_profile, se_df = read_cohort(abd_dir, os.path.join(outer_se, pheno, cohort))\n",
    "        crc_profile = read_cohort(abd_dir)\n",
    "        merged_crc_profile, crc_profile_tmp = merged_crc_profile.align(crc_profile, join='outer', fill_value=0)\n",
    "        merged_crc_profile.loc[crc_profile.index, crc_profile.columns] = crc_profile\n",
    "        \n",
    "    #random sampling sample and cluster \n",
    "    sub_slist = sampling_s(total_time, merged_crc_profile.index) \n",
    "    crc_profile = merged_crc_profile.loc[list(set(sub_slist)), ]\n",
    "    crc_profile = crc_profile[list(set(crc_profile.columns).intersection(set(d_df.index)))]\n",
    "    fr_dict = copy.deepcopy(get_ori_nets(crc_profile, d_df))\n",
    "    renamed_dict = {}\n",
    "    for sp in crc_profile.columns:\n",
    "        renamed_dict[sp] = sp.replace('_', '-')\n",
    "    crc_profile = crc_profile.rename(columns=renamed_dict)\n",
    "    \n",
    "    leaves = set(node_leaves[cluster]).intersection(set(crc_profile.columns))\n",
    "    leaves = sorted(list(leaves))\n",
    "    pairs = []\n",
    "    for i in range(len(leaves)):\n",
    "        for j in range(i+1, len(leaves)):\n",
    "            pairs.append((leaves[i], leaves[j]))\n",
    "    print('pairs: ', len(pairs))\n",
    "\n",
    "    remaining_pairs = []\n",
    "    for i in range(len(crc_profile.columns)):\n",
    "        s1 = crc_profile.columns[i]\n",
    "        for j in range(i+1, len(crc_profile.columns)):\n",
    "            s2 = crc_profile.columns[j]\n",
    "            if not ((s1 in leaves) and (s2 in leaves)):\n",
    "                remaining_pairs.append((s1, s2))\n",
    "    print('remaining pairs: ', len(remaining_pairs))\n",
    "    rand_dict = {}\n",
    "    high_dict = {}\n",
    "    low_dict = {}\n",
    "    for subid in range(len(sub_slist)):\n",
    "        sample = sub_slist[subid]\n",
    "        tmp_dict = {}\n",
    "        tmp_dict[sample] = copy.deepcopy(fr_dict[sample])\n",
    "        rand_df, in_df, out_df = select_edges_assign(fr_dict[sample], pairs, remaining_pairs, nsample)\n",
    "        rand_dict[sample] = copy.deepcopy(rand_df)\n",
    "        high_dict[sample] = copy.deepcopy(in_df)\n",
    "        low_dict[sample] = copy.deepcopy(out_df)\n",
    "    # rand_se = get_se_list(rand_dict, parent_dict, node_leaves, subtree_nodes, direct_children_dict)\n",
    "    # high_se = get_se_list(high_dict, parent_dict, node_leaves, subtree_nodes, direct_children_dict)\n",
    "    # low_se = get_se_list(low_dict, parent_dict, node_leaves, subtree_nodes, direct_children_dict)\n",
    "    # for  nid in rand_se.index:\n",
    "    #     result = result.append({'sample': nid, 'cluster': cluster, 'rand': rand_se.loc[nid, cluster], 'high': high_se.loc[nid, cluster], 'low': low_se.loc[nid, cluster]}, ignore_index=True)\n",
    "    \n",
    "    # tmp_dir = os.path.join(output_dir, pheno)\n",
    "    # if not os.path.exists(tmp_dir):\n",
    "    #     os.makedirs(tmp_dir)\n",
    "    # result.to_csv(os.path.join(tmp_dir, 'se_summary.tsv'), sep='\\t')\n",
    "    # pheno_result[pheno] = copy.deepcopy(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_se = get_se_list(rand_dict, parent_dict, node_leaves, subtree_nodes, direct_children_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_se = get_se_list(high_dict, parent_dict, node_leaves, subtree_nodes, direct_children_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_se = get_se_list(low_dict, parent_dict, node_leaves, subtree_nodes, direct_children_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand mean:  5.470405970535876 rand std:  0.0\n",
      "high mean:  7.540590931140479 high std:  0.0\n",
      "low mean:  3.2464544830797135 low std:  0.0\n",
      "rand vs high:  1.0\n",
      "rand vs low:  1.0\n",
      "high vs low:  1.0\n"
     ]
    }
   ],
   "source": [
    "summary(rand_se[cluster], high_se[cluster], low_se[cluster])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta_fr_r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
