{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd \n",
    "import random\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import fdrcorrection as fdr\n",
    "sys.path.append('../src')\n",
    "import abd_profile\n",
    "import numpy as np\n",
    "import se\n",
    "import tree_util\n",
    "import copy\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fr(d_df, profile, sname):\n",
    "    #print(profile.columns, d_df.index)\n",
    "    sp_list = sorted(list(set(profile.columns).intersection(set(d_df.index))))\n",
    "    #print(sp_list)\n",
    "    sp_d_df = d_df.loc[sp_list, sp_list]\n",
    "    #print(profile)\n",
    "    sp_profile = np.array(profile.loc[sname, sp_list])\n",
    "    value = np.dot(sp_profile.reshape(len(sp_profile), 1),sp_profile.reshape(1, len(sp_profile)))\n",
    "    width = value.shape[0]\n",
    "    cor_df = np.ones(shape=(width, width)) - sp_d_df.values\n",
    "    for i in range(width):\n",
    "        cor_df[i][i] = 0\n",
    "    value = np.multiply(value, cor_df)\n",
    "    fr_df = pd.DataFrame(value, index=sp_list, columns=sp_list)\n",
    "    return fr_df\n",
    "\n",
    "def shuffle_and_assign_dataframe(df, pairs, remaining_pairs):\n",
    "    new_df = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "    new_df = new_df.fillna(0)\n",
    "   \n",
    "    triu_indices = np.triu_indices_from(df, k=1)\n",
    "    upper_triangle_values = df.values[triu_indices]\n",
    "    \n",
    "    if len(pairs)>0:\n",
    "        n = len(pairs)\n",
    "        largest_n_values = sorted(upper_triangle_values, reverse=True)[:n]\n",
    "        last_values = sorted(upper_triangle_values, reverse=True)[n:]\n",
    "    else:\n",
    "        last_values = upper_triangle_values\n",
    "    shuffled_values = last_values.copy()\n",
    "    random.shuffle(shuffled_values)\n",
    "    for idx, (i, j) in enumerate(pairs):\n",
    "        if idx < len(pairs):\n",
    "            new_df.loc[i, j] = largest_n_values[idx]\n",
    "    \n",
    "    # print(len(remaining_pairs), len(shuffled_values))\n",
    "    for idx, (i, j) in enumerate(remaining_pairs):\n",
    "        new_df.loc[i, j] = shuffled_values[idx]\n",
    "    \n",
    "    df = new_df + new_df.T\n",
    "    \n",
    "    return df\n",
    "\n",
    "def shuffle_and_assign_dataframe_low(df, pairs, remaining_pairs):\n",
    "    new_df = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "    new_df = new_df.fillna(0)\n",
    "   \n",
    "    triu_indices = np.triu_indices_from(df, k=1)\n",
    "    upper_triangle_values = df.values[triu_indices]\n",
    "    \n",
    "    n = len(pairs)\n",
    "    largest_n_values = sorted(upper_triangle_values, reverse=True)[:n]\n",
    "    last_values = sorted(upper_triangle_values, reverse=True)[n:]\n",
    "    last_shuffled_values = last_values.copy()\n",
    "    \n",
    "    outside_need_n = len(remaining_pairs)\n",
    "    if outside_need_n > len(largest_n_values):\n",
    "        outside_values = largest_n_values + last_shuffled_values[:outside_need_n-len(largest_n_values)]\n",
    "        inside_values = last_shuffled_values[outside_need_n-len(largest_n_values):]\n",
    "    else:\n",
    "        outside_values = largest_n_values[:outside_need_n]\n",
    "        inside_values = largest_n_values[outside_need_n:]+last_shuffled_values\n",
    "    random.shuffle(outside_values)\n",
    "    random.shuffle(inside_values)\n",
    "    \n",
    "    \n",
    "    for idx, (i, j) in enumerate(pairs):\n",
    "        new_df.loc[i, j] = inside_values[idx]\n",
    "    \n",
    "    # print(len(remaining_pairs), len(shuffled_values))\n",
    "    for idx, (i, j) in enumerate(remaining_pairs):\n",
    "        new_df.loc[i, j] = outside_values[idx]\n",
    "    \n",
    "    df = new_df + new_df.T\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n: repeat time\n",
    "# k: sample number\n",
    "\n",
    "def summary(se_list_rand, se_list_high, se_list_low):\n",
    "    \n",
    "    # run paired Wilcoxon test\n",
    "    statistic, p1 = stats.wilcoxon(se_list_high, se_list_rand)\n",
    "    statistic, p2 = stats.wilcoxon(se_list_low, se_list_rand)\n",
    "    statistic, p3 = stats.wilcoxon(se_list_high, se_list_low)\n",
    "    print(\"rand mean: \", np.mean(se_list_rand), 'rand std: ', np.std(se_list_rand))\n",
    "    print(\"high mean: \", np.mean(se_list_high), 'high std: ', np.std(se_list_high))\n",
    "    print(\"low mean: \", np.mean(se_list_low), 'low std: ', np.std(se_list_low))\n",
    "    print(\"rand vs high: \", p1)\n",
    "    print(\"rand vs low: \", p2)\n",
    "    print(\"high vs low: \", p3)\n",
    "    df = pd.DataFrame(index=['rand', 'high', 'low'], columns=['mean', 'std'])\n",
    "    df.loc['rand'] = [np.mean(se_list_rand), np.std(se_list_rand)]\n",
    "    df.loc['high'] = [np.mean(se_list_high), np.std(se_list_high)]\n",
    "    df.loc['low'] = [np.mean(se_list_low), np.std(se_list_low)]\n",
    "    p_df = pd.DataFrame(index=['rand vs high', 'rand vs low', 'high vs low'], columns=['pvalue'])\n",
    "    p_df.loc['rand vs high'] = p1\n",
    "    p_df.loc['rand vs low'] = p2\n",
    "    p_df.loc['high vs low'] = p3\n",
    "    return df, p_df\n",
    "\n",
    "def get_ori_nets(profile, d_df):\n",
    "    net_dict = {}\n",
    "    rename_dict = {}\n",
    "    for sp in d_df.index:\n",
    "        rename_dict[sp] = sp.replace('_', '-')\n",
    "    renamed_d = d_df.rename(columns=rename_dict, index=rename_dict)\n",
    "\n",
    "    renamed_profile = profile.rename(columns=rename_dict)\n",
    "    renamed_profile = renamed_profile[list(set(renamed_profile.columns).intersection(set(renamed_d.index)))]\n",
    "    for sname in profile.index:\n",
    "        # print(sname)\n",
    "        edge_df = fr(renamed_d, renamed_profile, sname)\n",
    "        net_dict[sname] = copy.deepcopy(edge_df)\n",
    "    return net_dict\n",
    "\n",
    "def shuffle_nets(net_dict, pairs, remaining_pairs, n=1):\n",
    "    # first kind rand\n",
    "    rand_dict = {}\n",
    "    for sname in net_dict.keys():\n",
    "        rand_dict[sname] = []\n",
    "        for i in range(n):\n",
    "            rand_net = shuffle_and_assign_dataframe(net_dict[sname], [], remaining_pairs)\n",
    "            rand_dict[sname].append(copy.deepcopy(rand_net))\n",
    "    # second kind high\n",
    "    high_dict = {}\n",
    "    for sname in net_dict.keys():\n",
    "        high_dict[sname] = []\n",
    "        for i in range(n):\n",
    "            high_net = shuffle_and_assign_dataframe(net_dict[sname], pairs, remaining_pairs)\n",
    "            high_dict[sname].append(copy.deepcopy(high_net))\n",
    "\n",
    "    # third kind low\n",
    "    low_dict = {}\n",
    "    for sname in net_dict.keys():\n",
    "        low_dict[sname] = []\n",
    "        for i in range(n):\n",
    "            low_net = shuffle_and_assign_dataframe_low(net_dict[sname], pairs, remaining_pairs)\n",
    "            low_dict[sname].append(copy.deepcopy(low_net))\n",
    "    return rand_dict, high_dict, low_dict\n",
    "\n",
    "def split_newick(newick_tree):\n",
    "    json_tree = tree_util.parse(newick_tree)\n",
    "    largest = {'largest': 0}\n",
    "    leaf_list, l = tree_util.recu_compute(json_tree, 0, largest)\n",
    "    largest_level = largest['largest']\n",
    "    nlayer = largest_level\n",
    "    leaf_list, l = tree_util.recu_compute(json_tree, 0, largest)\n",
    "    layer_leaves_dict = tree_util.make_layer_dict(nlayer)\n",
    "\n",
    "    tree_util.recu_layer(json_tree, layer_leaves_dict)\n",
    "    tree_util.to_layer_leaves(layer_leaves_dict, nlayer)\n",
    "    result = {}\n",
    "    # compute leaf layer\n",
    "    result['leaves_dict'] = copy.deepcopy(layer_leaves_dict)\n",
    "    parent_dict = {}\n",
    "    tree_util.parents(json_tree, parent_dict)\n",
    "    node_leaves = {}\n",
    "    for level in layer_leaves_dict.keys():\n",
    "        for node, sp_list in layer_leaves_dict[level].items():\n",
    "            if node in node_leaves.keys():\n",
    "                continue\n",
    "            node_leaves[node] = copy.deepcopy(sp_list)\n",
    "    subtree_nodes = {}\n",
    "    for l in leaf_list:\n",
    "        parent = parent_dict[l]\n",
    "        if parent not in subtree_nodes.keys():\n",
    "            subtree_nodes[parent] = []\n",
    "        subtree_nodes[parent].append(l)\n",
    "\n",
    "    for node in node_leaves.keys():\n",
    "        parent = parent_dict[node]\n",
    "        if parent not in subtree_nodes.keys():\n",
    "            subtree_nodes[parent] = []\n",
    "        subtree_nodes[parent] += subtree_nodes[node]\n",
    "        subtree_nodes[parent].append(node)\n",
    "\n",
    "    for node in subtree_nodes.keys():\n",
    "        subtree_nodes[node].append(node)\n",
    "\n",
    "    direct_children_dict = {}\n",
    "    for node, parent in parent_dict.items():\n",
    "        if parent not in direct_children_dict:\n",
    "            direct_children_dict[parent] = []\n",
    "        direct_children_dict[parent].append(node)\n",
    "    return parent_dict, node_leaves, subtree_nodes, direct_children_dict\n",
    "\n",
    "def get_se_list(net_dict, parent_dict, node_leaves, subtree_nodes, direct_children_dict, param=0.5):\n",
    "    result = pd.DataFrame()\n",
    "    for sname in net_dict.keys():\n",
    "        for i, edge_df in enumerate(net_dict[sname]):\n",
    "            nid = \"{}_{}\".format(sname, i)\n",
    "            tmp = se.subtree_se_adj(edge_df, parent_dict, node_leaves, subtree_nodes, direct_children_dict, param)\n",
    "            for node in tmp.keys():\n",
    "                value = tmp[node]\n",
    "                result.loc[nid, node] = value\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_cohort(indir):\n",
    "    # Load the dataset\n",
    "    abd_path = os.path.join(indir, 'abd.tsv')\n",
    "    metadata_path = os.path.join(indir, 'metadata.tsv')\n",
    "    # first process data from gutmeta\n",
    "    raw_profile = abd_profile.input_profile(abd_path, transfer=True)\n",
    "    metadata = pd.read_csv(metadata_path, sep='\\t', header=0)\n",
    "    selected_raw_profile = raw_profile.loc[list(metadata['sample_id']), :]\n",
    "    selected_raw_profile = abd_profile.rename_s_level(selected_raw_profile)\n",
    "    crc_profile = abd_profile.clean(selected_raw_profile)\n",
    "\n",
    "    pheno_list = {}\n",
    "    for i in range(metadata.shape[0]):\n",
    "        pheno = metadata.loc[i, 'disease']\n",
    "        if pheno not in pheno_list.keys():\n",
    "            pheno_list[pheno] = []\n",
    "        if metadata.loc[i, 'sample_id'] in list(crc_profile.index):\n",
    "            pheno_list[pheno].append(metadata.loc[i, 'sample_id'])\n",
    "    return crc_profile\n",
    "    \n",
    "\n",
    "def select_clusters(diff_df, data_dir):\n",
    "    selected_diff = diff_df.loc[os.listdir(data_dir), ]\n",
    "    selected_diff.dropna(axis=1, thresh = len(selected_diff)/2 , inplace=True)\n",
    "    #num_df = (selected_diff < 0.05).sum(axis=0)\n",
    "    clusters = list(selected_diff.columns)\n",
    "    if 'root' in clusters:\n",
    "        clusters.remove('root')\n",
    "    return clusters\n",
    "\n",
    "def sampling(total, cluster_list, slist):\n",
    "    clusters = random.choices(cluster_list, k=total)\n",
    "    samples = random.choices(slist, k=total)\n",
    "    return samples, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_dir = '../data'\n",
    "total_sample = 100\n",
    "outer_se = '../result/large_scale_cohort'\n",
    "output_dir = '../result/validation/se_structure_simulation'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "plist = ['CRC']\n",
    "d_df = pd.read_csv('../data/sp_d.tsv', sep='\\t', header=0, index_col=0)\n",
    "diff_df = pd.read_csv(os.path.join(outer_se, 'p_all_cohorts_se.tsv'), sep='\\t', header=0, index_col=0)\n",
    "with open('../result/GCN_fix_tree/renamed_GCN_tree.newick') as fp:\n",
    "    newick_tree = fp.read()\n",
    "    parent_dict, node_leaves, subtree_nodes, direct_children_dict = split_newick(newick_tree)\n",
    "pheno_result= {}\n",
    "ori_se_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CRC ==\n",
      "-- CRC -- CRC1 --\n",
      "-- CRC -- CRC2 --\n",
      "-- CRC -- CRC3 --\n",
      "-- CRC -- CRC4 --\n",
      "-- CRC -- CRC5 --\n",
      "-- CRC -- CRC6 --\n",
      "-- CRC -- CRC7 --\n",
      "-- CRC -- CRC8 --\n",
      "-- CRC -- CRC9 --\n",
      "pairs:  2485\n",
      "remaining pairs:  379016\n",
      "pairs:  325\n",
      "remaining pairs:  381176\n",
      "pairs:  325\n",
      "remaining pairs:  381176\n",
      "pairs:  2628\n",
      "remaining pairs:  378873\n",
      "pairs:  990\n",
      "remaining pairs:  380511\n",
      "pairs:  3160\n",
      "remaining pairs:  378341\n",
      "pairs:  78\n",
      "remaining pairs:  381423\n",
      "pairs:  136\n",
      "remaining pairs:  381365\n",
      "pairs:  36\n",
      "remaining pairs:  381465\n",
      "pairs:  435\n",
      "remaining pairs:  381066\n",
      "pairs:  45\n",
      "remaining pairs:  381456\n",
      "pairs:  741\n",
      "remaining pairs:  380760\n",
      "pairs:  496\n",
      "remaining pairs:  381005\n",
      "pairs:  496\n",
      "remaining pairs:  381005\n",
      "pairs:  276\n",
      "remaining pairs:  381225\n",
      "pairs:  36\n",
      "remaining pairs:  381465\n",
      "pairs:  1225\n",
      "remaining pairs:  380276\n",
      "pairs:  191890\n",
      "remaining pairs:  189611\n",
      "pairs:  1891\n",
      "remaining pairs:  379610\n",
      "pairs:  78\n",
      "remaining pairs:  381423\n",
      "pairs:  105\n",
      "remaining pairs:  381396\n",
      "pairs:  78\n",
      "remaining pairs:  381423\n",
      "pairs:  55\n",
      "remaining pairs:  381446\n",
      "pairs:  45\n",
      "remaining pairs:  381456\n",
      "pairs:  861\n",
      "remaining pairs:  380640\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for pheno in plist:\n",
    "    result = pd.DataFrame(columns = ['sample', 'cluster', 'rand', 'high', 'low'])\n",
    "    #ori_se_dict[pheno] = {}\n",
    "    data_dir = os.path.join(outer_dir, pheno)\n",
    "    clusters = select_clusters(diff_df, data_dir)\n",
    "    se_dir = os.path.join(outer_se, pheno)\n",
    "    print('== {} =='.format(pheno))\n",
    "    #clusters = select_clusters(diff_df, data_dir)\n",
    "    merged_crc_profile = pd.DataFrame()\n",
    "    for cohort in os.listdir(data_dir):\n",
    "        print(f'-- {pheno} -- {cohort} --')\n",
    "        # sp_diff_df = pd.read_csv(os.path.join(abd_dff_dir, pheno, 'p_{}.tsv'.format(cohort)), sep='\\t', header=0, index_col=0)\n",
    "        se_cohort_dir = os.path.join(se_dir, cohort)\n",
    "        #ori_se = pd.read_csv(os.path.join(se_cohort_dir, 'se_Health.tsv'), sep='\\t', header=0, index_col=0)\n",
    "        abd_dir = os.path.join(data_dir, cohort)\n",
    "        #crc_profile, se_df = read_cohort(abd_dir, os.path.join(outer_se, pheno, cohort))\n",
    "        crc_profile = read_cohort(abd_dir)\n",
    "        merged_crc_profile, crc_profile_tmp = merged_crc_profile.align(crc_profile, join='outer', fill_value=0)\n",
    "        merged_crc_profile.loc[crc_profile.index, crc_profile.columns] = crc_profile\n",
    "    \n",
    "    #random sampling sample and cluster \n",
    "    sub_slist, sub_clusters = sampling(total_sample, clusters, merged_crc_profile.index)  \n",
    "    sample_df = pd.DataFrame(columns=['sample', 'cluster'])\n",
    "    sample_df['sample'] = sub_slist\n",
    "    sample_df['cluster'] = sub_clusters\n",
    "    take_clusters = list(set(sub_clusters))\n",
    "\n",
    "    crc_profile = merged_crc_profile.loc[list(set(sub_slist)), ]\n",
    "    crc_profile = crc_profile[list(set(crc_profile.columns).intersection(set(d_df.index)))]\n",
    "    fr_dict = copy.deepcopy(get_ori_nets(crc_profile, d_df))\n",
    "    renamed_dict = {}\n",
    "    for sp in crc_profile.columns:\n",
    "        renamed_dict[sp] = sp.replace('_', '-')\n",
    "    crc_profile = crc_profile.rename(columns=renamed_dict)\n",
    "\n",
    "    for tmp_cluster in take_clusters:\n",
    "        leaves = set(node_leaves[tmp_cluster]).intersection(set(crc_profile.columns))\n",
    "        leaves = sorted(list(leaves))\n",
    "        pairs = []\n",
    "        for i in range(len(leaves)):\n",
    "            for j in range(i+1, len(leaves)):\n",
    "                pairs.append((leaves[i], leaves[j]))\n",
    "        print('pairs: ', len(pairs))\n",
    "\n",
    "        remaining_pairs = []\n",
    "        for i in range(len(crc_profile.columns)):\n",
    "            s1 = crc_profile.columns[i]\n",
    "            for j in range(i+1, len(crc_profile.columns)):\n",
    "                s2 = crc_profile.columns[j]\n",
    "                if not ((s1 in leaves) and (s2 in leaves)):\n",
    "                    remaining_pairs.append((s1, s2))\n",
    "        print('remaining pairs: ', len(remaining_pairs))\n",
    "\n",
    "        tmp_df = sample_df[sample_df['cluster'] == tmp_cluster]\n",
    "        for sample in tmp_df['sample']:\n",
    "            tmp_dict = {}\n",
    "            tmp_dict[sample] = copy.deepcopy(fr_dict[sample])\n",
    "            rand_dict, high_dict, low_dict = shuffle_nets(tmp_dict, pairs, remaining_pairs, 1)\n",
    "            rand_se = get_se_list(rand_dict, parent_dict, node_leaves, subtree_nodes, direct_children_dict)\n",
    "            high_se = get_se_list(high_dict, parent_dict, node_leaves, subtree_nodes, direct_children_dict)\n",
    "            low_se = get_se_list(low_dict, parent_dict, node_leaves, subtree_nodes, direct_children_dict)\n",
    "            for  nid in rand_se.index:\n",
    "                result = result.append({'sample': nid, 'cluster': tmp_cluster, 'rand': rand_se.loc[nid, tmp_cluster], 'high': high_se.loc[nid, tmp_cluster], 'low': low_se.loc[nid, tmp_cluster]}, ignore_index=True)\n",
    "\n",
    "    tmp_dir = os.path.join(output_dir, pheno)\n",
    "    if not os.path.exists(tmp_dir):\n",
    "        os.makedirs(tmp_dir)\n",
    "    result.to_csv(os.path.join(tmp_dir, 'se_summary.tsv'), sep='\\t')\n",
    "    pheno_result[pheno] = copy.deepcopy(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand mean:  0.32876340479264526 rand std:  0.48543600464450826\n",
      "high mean:  3.4978149723563536 high std:  1.0174569051290732\n",
      "low mean:  0.33227911354466755 low std:  0.5687596463086205\n",
      "rand vs high:  3.896559845095909e-18\n",
      "rand vs low:  0.12515445955953292\n",
      "high vs low:  3.896559845095909e-18\n"
     ]
    }
   ],
   "source": [
    "for pheno in plist:\n",
    "    tmp_dir = os.path.join(output_dir, pheno)\n",
    "    result = pheno_result[pheno]\n",
    "    rand_se = result['rand']\n",
    "    high_se = result['high']\n",
    "    low_se = result['low']\n",
    "    df, p_df = summary(rand_se, high_se, low_se)\n",
    "    df.to_csv(os.path.join(tmp_dir, 'se_mean_std.tsv'), sep='\\t')\n",
    "    p_df.to_csv(os.path.join(tmp_dir, 'se_p_values.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta_fr_r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
