{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import argparse\n",
    "import os\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "\n",
    "metadata_df = pd.read_csv('../data/IBD/IBD1/metadata.tsv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增强版元数据统计函数\n",
    "def enhanced_column_stats(df):\n",
    "    results = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # 基本信息\n",
    "        na_count = df[col].isna().sum()\n",
    "        na_percentage = (na_count / len(df)) * 100\n",
    "        dtype = df[col].dtype\n",
    "        \n",
    "        # 确定数据类型\n",
    "        if pd.api.types.is_numeric_dtype(dtype):\n",
    "            unique_values = df[col].dropna().unique()\n",
    "            if len(unique_values) <= 10:\n",
    "                data_type = 'Categorical'\n",
    "            else:\n",
    "                data_type = 'Numerical'\n",
    "        else:\n",
    "            data_type = 'Categorical'\n",
    "            \n",
    "        # 数据分布统计\n",
    "        if data_type == 'Numerical':\n",
    "            # 数值变量统计\n",
    "            mean_val = df[col].mean()\n",
    "            median_val = df[col].median()\n",
    "            std_val = df[col].std()\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            \n",
    "            # 检查数据偏斜度\n",
    "            from scipy import stats as scipystats\n",
    "            skewness = scipystats.skew(df[col].dropna())\n",
    "            \n",
    "            # 检查数据范围和分布，帮助识别可能需要转换的变量\n",
    "            summary = f\"Mean={mean_val:.2f}, Median={median_val:.2f}, SD={std_val:.2f}, Range={min_val:.2f}-{max_val:.2f}, Skew={skewness:.2f}\"\n",
    "            \n",
    "            # 计算变异系数(CV)，帮助判断变量的离散程度\n",
    "            cv = std_val / mean_val if mean_val != 0 else float('inf')\n",
    "            \n",
    "            # 如果CV很大，可能不适合作为直接的混杂因素，可能需要转换\n",
    "            cv_comment = \"High Variability\" if abs(cv) > 1 else \"Normal Variability\"\n",
    "            \n",
    "        else:\n",
    "            # 分类变量统计\n",
    "            value_counts = df[col].value_counts()\n",
    "            n_categories = len(value_counts)\n",
    "            most_common = value_counts.index[0] if not value_counts.empty else \"N/A\"\n",
    "            most_common_count = value_counts.iloc[0] if not value_counts.empty else 0\n",
    "            most_common_pct = (most_common_count / df[col].count()) * 100\n",
    "            \n",
    "            # 类别平衡性检查 - 不平衡的分类变量可能导致模型问题\n",
    "            balance_ratio = value_counts.min() / value_counts.max() if len(value_counts) > 1 and value_counts.max() > 0 else 0\n",
    "            balance_comment = \"Imbalanced\" if balance_ratio < 0.1 else \"Balanced\"\n",
    "            \n",
    "            summary = f\"Categories={n_categories}, Most common='{most_common}' ({most_common_pct:.1f}%), Balance={balance_comment}\"\n",
    "            cv = None\n",
    "            cv_comment = None\n",
    "        \n",
    "        # 添加相关性分析 - 只对数值型变量执行\n",
    "        correlations = {}\n",
    "        if data_type == 'Numerical':\n",
    "            for other_col in df.select_dtypes(include=['number']).columns:\n",
    "                if other_col != col:\n",
    "                    # 计算与其他数值变量的相关性\n",
    "                    correlation = df[[col, other_col]].dropna().corr().iloc[0, 1]\n",
    "                    if abs(correlation) > 0.3:  # 只保存中等及以上相关性\n",
    "                        correlations[other_col] = correlation\n",
    "        \n",
    "        # 是否推荐作为混杂因素的判断标准\n",
    "        recommended = False\n",
    "        recommendation_reason = []\n",
    "        \n",
    "        # 缺失值不能太多\n",
    "        if na_percentage > 30:\n",
    "            recommendation_reason.append(f\"High missing data ({na_percentage:.1f}%)\")\n",
    "        \n",
    "        # 数值变量评估\n",
    "        elif data_type == 'Numerical':\n",
    "            # 如果变异系数过大，可能需要转换\n",
    "            if abs(cv) > 3:\n",
    "                recommendation_reason.append(\"Extreme variability (consider transformation)\")\n",
    "            # 如果偏斜度过大，可能需要转换\n",
    "            elif abs(skewness) > 3:\n",
    "                recommendation_reason.append(\"Highly skewed (consider transformation)\")\n",
    "            else:\n",
    "                recommended = True\n",
    "        \n",
    "        # 分类变量评估\n",
    "        elif data_type == 'Categorical':\n",
    "            # 太多类别的分类变量不适合\n",
    "            if n_categories > 20:\n",
    "                recommendation_reason.append(f\"Too many categories ({n_categories})\")\n",
    "            # 极度不平衡的分类\n",
    "            elif balance_ratio < 0.01 and n_categories > 1:\n",
    "                recommendation_reason.append(\"Extremely imbalanced\")\n",
    "            # 单一类别占比过高\n",
    "            elif most_common_pct > 95:\n",
    "                recommendation_reason.append(f\"Dominant category ({most_common_pct:.1f}%)\")\n",
    "            else:\n",
    "                recommended = True\n",
    "        \n",
    "        if not recommendation_reason:\n",
    "            recommendation_reason.append(\"Suitable as confounder\")\n",
    "        \n",
    "        results.append({\n",
    "            'Column': col,\n",
    "            'Data Type': data_type,\n",
    "            'NA Count': na_count,\n",
    "            'NA Percentage': na_percentage,\n",
    "            'Summary': summary,\n",
    "            'Strong Correlations': str(correlations) if correlations else \"None\",\n",
    "            'Recommended as Confounder': recommended,\n",
    "            'Recommendation Reason': \"; \".join(recommendation_reason)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def process_metadata_files(df, outpath=None):\n",
    "    \"\"\"\n",
    "    处理元数据并输出统计信息，打印推荐的混杂因素\n",
    "    \n",
    "    参数:\n",
    "    df: 包含元数据的DataFrame\n",
    "    outpath: 可选，如果提供则将统计结果保存到该路径\n",
    "    \n",
    "    返回:\n",
    "    stats: 包含统计信息的DataFrame\n",
    "    \"\"\"\n",
    "    # 获取增强的统计信息\n",
    "    stats = enhanced_column_stats(df)\n",
    "    stats = stats[stats['NA Count'] != len(df)]  # 移除全部为NA的列\n",
    "    \n",
    "    # 如果提供了输出路径，保存统计结果\n",
    "    if outpath:\n",
    "        stats.to_csv(outpath, sep='\\t', index=False)\n",
    "    \n",
    "    # 获取推荐的混杂因素列表\n",
    "    recommended_confounders = stats[stats['Recommended as Confounder'] == True]['Column'].tolist()\n",
    "    recommended_numerical = stats[(stats['Recommended as Confounder'] == True) & \n",
    "                                 (stats['Data Type'] == 'Numerical')]['Column'].tolist()\n",
    "    recommended_categorical = stats[(stats['Recommended as Confounder'] == True) & \n",
    "                                   (stats['Data Type'] == 'Categorical')]['Column'].tolist()\n",
    "    \n",
    "    # 打印推荐的混杂因素\n",
    "    print(\"\\n===== RECOMMENDED CONFOUNDERS =====\")\n",
    "    print(f\"\\nAll recommended confounders ({len(recommended_confounders)}):\")\n",
    "    print(\", \".join(recommended_confounders))\n",
    "    \n",
    "    print(f\"\\nRecommended numerical confounders ({len(recommended_numerical)}):\")\n",
    "    print(\", \".join(recommended_numerical))\n",
    "    \n",
    "    print(f\"\\nRecommended categorical confounders ({len(recommended_categorical)}):\")\n",
    "    print(\", \".join(recommended_categorical))\n",
    "    \n",
    "    # 打印不推荐的变量及原因\n",
    "    not_recommended = stats[stats['Recommended as Confounder'] == False]\n",
    "    print(f\"\\nVariables NOT recommended as confounders ({len(not_recommended)}):\")\n",
    "    for _, row in not_recommended.iterrows():\n",
    "        print(f\"- {row['Column']}: {row['Recommendation Reason']}\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== RECOMMENDED CONFOUNDERS =====\n",
      "\n",
      "All recommended confounders (12):\n",
      "study_condition, disease, age, age_category, gender, country, number_reads, number_bases, minimum_read_length, median_read_length, BMI, mgs_richness\n",
      "\n",
      "Recommended numerical confounders (6):\n",
      "age, number_reads, number_bases, median_read_length, BMI, mgs_richness\n",
      "\n",
      "Recommended categorical confounders (6):\n",
      "study_condition, disease, age_category, gender, country, minimum_read_length\n",
      "\n",
      "Variables NOT recommended as confounders (12):\n",
      "- study_name: Dominant category (100.0%)\n",
      "- sample_id: Too many categories (393)\n",
      "- subject_id: Too many categories (316)\n",
      "- body_site: Dominant category (100.0%)\n",
      "- non_westernized: Dominant category (100.0%)\n",
      "- sequencing_platform: Dominant category (100.0%)\n",
      "- PMID: Dominant category (100.0%)\n",
      "- NCBI_accession: Too many categories (393)\n",
      "- curator: Dominant category (100.0%)\n",
      "- days_from_first_collection: Highly skewed (consider transformation)\n",
      "- disease_subtype: High missing data (62.8%)\n",
      "- ferm_milk_prod_consumer: High missing data (95.2%)\n"
     ]
    }
   ],
   "source": [
    "stat = process_metadata_files(metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>NA Count</th>\n",
       "      <th>NA Percentage</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Strong Correlations</th>\n",
       "      <th>Recommended as Confounder</th>\n",
       "      <th>Recommendation Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>study_name</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=1, Most common='NielsenHB_2014' (10...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Dominant category (100.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample_id</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=393, Most common='MH0001' (0.3%), B...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Too many categories (393)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject_id</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=316, Most common='O2_UC24' (0.5%), ...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Too many categories (316)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>body_site</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=1, Most common='stool' (100.0%), Ba...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Dominant category (100.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>study_condition</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=2, Most common='control' (62.8%), B...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>disease</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=2, Most common='Health' (62.8%), Ba...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>age</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>2</td>\n",
       "      <td>0.508906</td>\n",
       "      <td>Mean=47.69, Median=49.00, SD=12.75, Range=19.0...</td>\n",
       "      <td>{'minimum_read_length': 0.542394161007958, 'BM...</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>age_category</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=2, Most common='adult' (94.9%), Bal...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gender</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=2, Most common='female' (57.0%), Ba...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>country</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=2, Most common='ESP' (55.0%), Balan...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>non_westernized</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=1, Most common='no' (100.0%), Balan...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Dominant category (100.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sequencing_platform</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=1, Most common='IlluminaHiSeq' (100...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Dominant category (100.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PMID</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=1, Most common='24997787' (100.0%),...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Dominant category (100.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>number_reads</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Mean=56663647.93, Median=56991169.00, SD=19442...</td>\n",
       "      <td>{'number_bases': 0.9784825159176457, 'median_r...</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>number_bases</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Mean=4091226718.01, Median=3994482577.00, SD=1...</td>\n",
       "      <td>{'number_reads': 0.9784825159176457, 'median_r...</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>minimum_read_length</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=6, Most common='30' (55.0%), Balanc...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>median_read_length</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Mean=72.75, Median=74.00, SD=8.80, Range=44.00...</td>\n",
       "      <td>{'number_reads': 0.5027573829522167, 'number_b...</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NCBI_accession</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=393, Most common='ERR209908;ERR2099...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Too many categories (393)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>curator</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=1, Most common='Paolo_Manghi' (100....</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Dominant category (100.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>BMI</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>1</td>\n",
       "      <td>0.254453</td>\n",
       "      <td>Mean=26.52, Median=25.40, SD=5.49, Range=17.10...</td>\n",
       "      <td>{'age': 0.3837484379799608, 'minimum_read_leng...</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>days_from_first_collection</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Mean=32.18, Median=0.00, SD=85.56, Range=0.00-...</td>\n",
       "      <td>{'number_bases': 0.3036052998925406, 'minimum_...</td>\n",
       "      <td>False</td>\n",
       "      <td>Highly skewed (consider transformation)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>disease_subtype</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>247</td>\n",
       "      <td>62.849873</td>\n",
       "      <td>Categories=2, Most common='UC' (87.0%), Balanc...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>High missing data (62.8%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>mgs_richness</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>3</td>\n",
       "      <td>0.763359</td>\n",
       "      <td>Mean=155.16, Median=156.00, SD=46.63, Range=33...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>ferm_milk_prod_consumer</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>374</td>\n",
       "      <td>95.165394</td>\n",
       "      <td>Categories=1, Most common='dfmp' (100.0%), Bal...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>High missing data (95.2%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Column    Data Type  NA Count  NA Percentage  \\\n",
       "0                    study_name  Categorical         0       0.000000   \n",
       "1                     sample_id  Categorical         0       0.000000   \n",
       "2                    subject_id  Categorical         0       0.000000   \n",
       "3                     body_site  Categorical         0       0.000000   \n",
       "5               study_condition  Categorical         0       0.000000   \n",
       "6                       disease  Categorical         0       0.000000   \n",
       "7                           age    Numerical         2       0.508906   \n",
       "9                  age_category  Categorical         0       0.000000   \n",
       "10                       gender  Categorical         0       0.000000   \n",
       "11                      country  Categorical         0       0.000000   \n",
       "12              non_westernized  Categorical         0       0.000000   \n",
       "13          sequencing_platform  Categorical         0       0.000000   \n",
       "15                         PMID  Categorical         0       0.000000   \n",
       "16                 number_reads    Numerical         0       0.000000   \n",
       "17                 number_bases    Numerical         0       0.000000   \n",
       "18          minimum_read_length  Categorical         0       0.000000   \n",
       "19           median_read_length    Numerical         0       0.000000   \n",
       "20               NCBI_accession  Categorical         0       0.000000   \n",
       "23                      curator  Categorical         0       0.000000   \n",
       "24                          BMI    Numerical         1       0.254453   \n",
       "26   days_from_first_collection    Numerical         0       0.000000   \n",
       "37              disease_subtype  Categorical       247      62.849873   \n",
       "110                mgs_richness    Numerical         3       0.763359   \n",
       "111     ferm_milk_prod_consumer  Categorical       374      95.165394   \n",
       "\n",
       "                                               Summary  \\\n",
       "0    Categories=1, Most common='NielsenHB_2014' (10...   \n",
       "1    Categories=393, Most common='MH0001' (0.3%), B...   \n",
       "2    Categories=316, Most common='O2_UC24' (0.5%), ...   \n",
       "3    Categories=1, Most common='stool' (100.0%), Ba...   \n",
       "5    Categories=2, Most common='control' (62.8%), B...   \n",
       "6    Categories=2, Most common='Health' (62.8%), Ba...   \n",
       "7    Mean=47.69, Median=49.00, SD=12.75, Range=19.0...   \n",
       "9    Categories=2, Most common='adult' (94.9%), Bal...   \n",
       "10   Categories=2, Most common='female' (57.0%), Ba...   \n",
       "11   Categories=2, Most common='ESP' (55.0%), Balan...   \n",
       "12   Categories=1, Most common='no' (100.0%), Balan...   \n",
       "13   Categories=1, Most common='IlluminaHiSeq' (100...   \n",
       "15   Categories=1, Most common='24997787' (100.0%),...   \n",
       "16   Mean=56663647.93, Median=56991169.00, SD=19442...   \n",
       "17   Mean=4091226718.01, Median=3994482577.00, SD=1...   \n",
       "18   Categories=6, Most common='30' (55.0%), Balanc...   \n",
       "19   Mean=72.75, Median=74.00, SD=8.80, Range=44.00...   \n",
       "20   Categories=393, Most common='ERR209908;ERR2099...   \n",
       "23   Categories=1, Most common='Paolo_Manghi' (100....   \n",
       "24   Mean=26.52, Median=25.40, SD=5.49, Range=17.10...   \n",
       "26   Mean=32.18, Median=0.00, SD=85.56, Range=0.00-...   \n",
       "37   Categories=2, Most common='UC' (87.0%), Balanc...   \n",
       "110  Mean=155.16, Median=156.00, SD=46.63, Range=33...   \n",
       "111  Categories=1, Most common='dfmp' (100.0%), Bal...   \n",
       "\n",
       "                                   Strong Correlations  \\\n",
       "0                                                 None   \n",
       "1                                                 None   \n",
       "2                                                 None   \n",
       "3                                                 None   \n",
       "5                                                 None   \n",
       "6                                                 None   \n",
       "7    {'minimum_read_length': 0.542394161007958, 'BM...   \n",
       "9                                                 None   \n",
       "10                                                None   \n",
       "11                                                None   \n",
       "12                                                None   \n",
       "13                                                None   \n",
       "15                                                None   \n",
       "16   {'number_bases': 0.9784825159176457, 'median_r...   \n",
       "17   {'number_reads': 0.9784825159176457, 'median_r...   \n",
       "18                                                None   \n",
       "19   {'number_reads': 0.5027573829522167, 'number_b...   \n",
       "20                                                None   \n",
       "23                                                None   \n",
       "24   {'age': 0.3837484379799608, 'minimum_read_leng...   \n",
       "26   {'number_bases': 0.3036052998925406, 'minimum_...   \n",
       "37                                                None   \n",
       "110                                               None   \n",
       "111                                               None   \n",
       "\n",
       "     Recommended as Confounder                    Recommendation Reason  \n",
       "0                        False               Dominant category (100.0%)  \n",
       "1                        False                Too many categories (393)  \n",
       "2                        False                Too many categories (316)  \n",
       "3                        False               Dominant category (100.0%)  \n",
       "5                         True                   Suitable as confounder  \n",
       "6                         True                   Suitable as confounder  \n",
       "7                         True                   Suitable as confounder  \n",
       "9                         True                   Suitable as confounder  \n",
       "10                        True                   Suitable as confounder  \n",
       "11                        True                   Suitable as confounder  \n",
       "12                       False               Dominant category (100.0%)  \n",
       "13                       False               Dominant category (100.0%)  \n",
       "15                       False               Dominant category (100.0%)  \n",
       "16                        True                   Suitable as confounder  \n",
       "17                        True                   Suitable as confounder  \n",
       "18                        True                   Suitable as confounder  \n",
       "19                        True                   Suitable as confounder  \n",
       "20                       False                Too many categories (393)  \n",
       "23                       False               Dominant category (100.0%)  \n",
       "24                        True                   Suitable as confounder  \n",
       "26                       False  Highly skewed (consider transformation)  \n",
       "37                       False                High missing data (62.8%)  \n",
       "110                       True                   Suitable as confounder  \n",
       "111                      False                High missing data (95.2%)  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>NA Count</th>\n",
       "      <th>NA Percentage</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Strong Correlations</th>\n",
       "      <th>Recommended as Confounder</th>\n",
       "      <th>Recommendation Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>study_condition</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=2, Most common='control' (62.8%), B...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>disease</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=2, Most common='Health' (62.8%), Ba...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>age</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>2</td>\n",
       "      <td>0.508906</td>\n",
       "      <td>Mean=47.69, Median=49.00, SD=12.75, Range=19.0...</td>\n",
       "      <td>{'minimum_read_length': 0.542394161007958, 'BM...</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>age_category</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=2, Most common='adult' (94.9%), Bal...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gender</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=2, Most common='female' (57.0%), Ba...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>country</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=2, Most common='ESP' (55.0%), Balan...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>number_reads</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Mean=56663647.93, Median=56991169.00, SD=19442...</td>\n",
       "      <td>{'number_bases': 0.9784825159176457, 'median_r...</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>number_bases</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Mean=4091226718.01, Median=3994482577.00, SD=1...</td>\n",
       "      <td>{'number_reads': 0.9784825159176457, 'median_r...</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>minimum_read_length</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Categories=6, Most common='30' (55.0%), Balanc...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>median_read_length</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Mean=72.75, Median=74.00, SD=8.80, Range=44.00...</td>\n",
       "      <td>{'number_reads': 0.5027573829522167, 'number_b...</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>BMI</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>1</td>\n",
       "      <td>0.254453</td>\n",
       "      <td>Mean=26.52, Median=25.40, SD=5.49, Range=17.10...</td>\n",
       "      <td>{'age': 0.3837484379799608, 'minimum_read_leng...</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>mgs_richness</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>3</td>\n",
       "      <td>0.763359</td>\n",
       "      <td>Mean=155.16, Median=156.00, SD=46.63, Range=33...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>Suitable as confounder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Column    Data Type  NA Count  NA Percentage  \\\n",
       "5        study_condition  Categorical         0       0.000000   \n",
       "6                disease  Categorical         0       0.000000   \n",
       "7                    age    Numerical         2       0.508906   \n",
       "9           age_category  Categorical         0       0.000000   \n",
       "10                gender  Categorical         0       0.000000   \n",
       "11               country  Categorical         0       0.000000   \n",
       "16          number_reads    Numerical         0       0.000000   \n",
       "17          number_bases    Numerical         0       0.000000   \n",
       "18   minimum_read_length  Categorical         0       0.000000   \n",
       "19    median_read_length    Numerical         0       0.000000   \n",
       "24                   BMI    Numerical         1       0.254453   \n",
       "110         mgs_richness    Numerical         3       0.763359   \n",
       "\n",
       "                                               Summary  \\\n",
       "5    Categories=2, Most common='control' (62.8%), B...   \n",
       "6    Categories=2, Most common='Health' (62.8%), Ba...   \n",
       "7    Mean=47.69, Median=49.00, SD=12.75, Range=19.0...   \n",
       "9    Categories=2, Most common='adult' (94.9%), Bal...   \n",
       "10   Categories=2, Most common='female' (57.0%), Ba...   \n",
       "11   Categories=2, Most common='ESP' (55.0%), Balan...   \n",
       "16   Mean=56663647.93, Median=56991169.00, SD=19442...   \n",
       "17   Mean=4091226718.01, Median=3994482577.00, SD=1...   \n",
       "18   Categories=6, Most common='30' (55.0%), Balanc...   \n",
       "19   Mean=72.75, Median=74.00, SD=8.80, Range=44.00...   \n",
       "24   Mean=26.52, Median=25.40, SD=5.49, Range=17.10...   \n",
       "110  Mean=155.16, Median=156.00, SD=46.63, Range=33...   \n",
       "\n",
       "                                   Strong Correlations  \\\n",
       "5                                                 None   \n",
       "6                                                 None   \n",
       "7    {'minimum_read_length': 0.542394161007958, 'BM...   \n",
       "9                                                 None   \n",
       "10                                                None   \n",
       "11                                                None   \n",
       "16   {'number_bases': 0.9784825159176457, 'median_r...   \n",
       "17   {'number_reads': 0.9784825159176457, 'median_r...   \n",
       "18                                                None   \n",
       "19   {'number_reads': 0.5027573829522167, 'number_b...   \n",
       "24   {'age': 0.3837484379799608, 'minimum_read_leng...   \n",
       "110                                               None   \n",
       "\n",
       "     Recommended as Confounder   Recommendation Reason  \n",
       "5                         True  Suitable as confounder  \n",
       "6                         True  Suitable as confounder  \n",
       "7                         True  Suitable as confounder  \n",
       "9                         True  Suitable as confounder  \n",
       "10                        True  Suitable as confounder  \n",
       "11                        True  Suitable as confounder  \n",
       "16                        True  Suitable as confounder  \n",
       "17                        True  Suitable as confounder  \n",
       "18                        True  Suitable as confounder  \n",
       "19                        True  Suitable as confounder  \n",
       "24                        True  Suitable as confounder  \n",
       "110                       True  Suitable as confounder  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat[stat[\"Recommended as Confounder\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigensp_df = pd.read_csv('../result/large_scale_cohort/IBD/IBD1/eigenspecies/IBD1.eigenspecies.csv',index_col=0,header=0,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from scipy.stats import pearsonr, skew\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def enhanced_column_stats(df):\n",
    "    \"\"\"\n",
    "    为DataFrame中的每一列计算增强统计信息，判断是否适合作为混杂因素\n",
    "    \n",
    "    参数:\n",
    "    df: 包含元数据的DataFrame\n",
    "    \n",
    "    返回:\n",
    "    stats_df: 包含统计信息的DataFrame\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # 基本信息\n",
    "        na_count = df[col].isna().sum()\n",
    "        na_percentage = (na_count / len(df)) * 100\n",
    "        dtype = df[col].dtype\n",
    "        \n",
    "        # 确定数据类型\n",
    "        if pd.api.types.is_numeric_dtype(dtype):\n",
    "            unique_values = df[col].dropna().unique()\n",
    "            if len(unique_values) <= 10:\n",
    "                data_type = 'Categorical'\n",
    "            else:\n",
    "                data_type = 'Numerical'\n",
    "        else:\n",
    "            data_type = 'Categorical'\n",
    "            \n",
    "        # 数据分布统计\n",
    "        if data_type == 'Numerical':\n",
    "            # 数值变量统计\n",
    "            non_na_values = df[col].dropna()\n",
    "            if len(non_na_values) > 0:\n",
    "                mean_val = non_na_values.mean()\n",
    "                median_val = non_na_values.median()\n",
    "                std_val = non_na_values.std()\n",
    "                min_val = non_na_values.min()\n",
    "                max_val = non_na_values.max()\n",
    "                \n",
    "                # 检查数据偏斜度\n",
    "                skewness = skew(non_na_values) if len(non_na_values) > 2 else 0\n",
    "                \n",
    "                # 检查数据范围和分布\n",
    "                summary = f\"Mean={mean_val:.2f}, Median={median_val:.2f}, SD={std_val:.2f}, Range={min_val:.2f}-{max_val:.2f}, Skew={skewness:.2f}\"\n",
    "                \n",
    "                # 计算变异系数(CV)\n",
    "                cv = std_val / abs(mean_val) if mean_val != 0 else float('inf')\n",
    "                cv_comment = \"High Variability\" if abs(cv) > 1 else \"Normal Variability\"\n",
    "            else:\n",
    "                summary = \"No valid data\"\n",
    "                cv = float('inf')\n",
    "                cv_comment = \"No data\"\n",
    "                skewness = 0\n",
    "                \n",
    "        else:\n",
    "            # 分类变量统计\n",
    "            value_counts = df[col].dropna().value_counts()\n",
    "            n_categories = len(value_counts)\n",
    "            \n",
    "            if not value_counts.empty:\n",
    "                most_common = value_counts.index[0]\n",
    "                most_common_count = value_counts.iloc[0]\n",
    "                most_common_pct = (most_common_count / df[col].count()) * 100\n",
    "                \n",
    "                # 类别平衡性检查\n",
    "                balance_ratio = value_counts.min() / value_counts.max() if len(value_counts) > 1 and value_counts.max() > 0 else 0\n",
    "                balance_comment = \"Imbalanced\" if balance_ratio < 0.1 else \"Balanced\"\n",
    "                \n",
    "                summary = f\"Categories={n_categories}, Most common='{most_common}' ({most_common_pct:.1f}%), Balance={balance_comment}\"\n",
    "            else:\n",
    "                summary = \"No valid data\"\n",
    "                balance_ratio = 0\n",
    "                \n",
    "            cv = None\n",
    "            cv_comment = None\n",
    "            skewness = None\n",
    "        \n",
    "        # 添加相关性分析 - 只对数值型变量执行\n",
    "        correlations = {}\n",
    "        if data_type == 'Numerical':\n",
    "            for other_col in df.select_dtypes(include=['number']).columns:\n",
    "                if other_col != col:\n",
    "                    # 计算与其他数值变量的相关性\n",
    "                    corr_data = df[[col, other_col]].dropna()\n",
    "                    if len(corr_data) > 5:  # 确保有足够的数据点\n",
    "                        correlation = corr_data.corr().iloc[0, 1]\n",
    "                        if abs(correlation) > 0.3:  # 只保存中等及以上相关性\n",
    "                            correlations[other_col] = correlation\n",
    "        \n",
    "        # 是否推荐作为混杂因素的判断标准\n",
    "        recommended = False\n",
    "        recommendation_reason = []\n",
    "        \n",
    "        # 排除样本ID和一些特定的非混杂列\n",
    "        exclude_cols = ['sample_id', 'subject_id', 'NCBI_accession', 'PMID', 'curator']\n",
    "        if col in exclude_cols:\n",
    "            recommendation_reason.append(\"Identifier or metadata column\")\n",
    "        \n",
    "        # 缺失值不能太多\n",
    "        elif na_percentage > 30:\n",
    "            recommendation_reason.append(f\"High missing data ({na_percentage:.1f}%)\")\n",
    "        \n",
    "        # 数值变量评估\n",
    "        elif data_type == 'Numerical':\n",
    "            # 如果变异系数过大或数据极度偏斜，可能需要转换\n",
    "            if cv is not None and abs(cv) > 3:\n",
    "                recommendation_reason.append(\"Extreme variability\")\n",
    "            elif skewness is not None and abs(skewness) > 3:\n",
    "                recommendation_reason.append(\"Highly skewed\")\n",
    "            else:\n",
    "                recommended = True\n",
    "        \n",
    "        # 分类变量评估\n",
    "        elif data_type == 'Categorical':\n",
    "            # 太多类别的分类变量不适合\n",
    "            if n_categories > 20:\n",
    "                recommendation_reason.append(f\"Too many categories ({n_categories})\")\n",
    "            # 极度不平衡的分类\n",
    "            elif balance_ratio < 0.01 and n_categories > 1:\n",
    "                recommendation_reason.append(\"Extremely imbalanced\")\n",
    "            # 单一类别占比过高\n",
    "            elif 'most_common_pct' in locals() and most_common_pct > 95:\n",
    "                recommendation_reason.append(f\"Dominant category ({most_common_pct:.1f}%)\")\n",
    "            else:\n",
    "                recommended = True\n",
    "        \n",
    "        if not recommendation_reason:\n",
    "            recommendation_reason.append(\"Suitable as confounder\")\n",
    "        \n",
    "        results.append({\n",
    "            'Column': col,\n",
    "            'Data Type': data_type,\n",
    "            'NA Count': na_count,\n",
    "            'NA Percentage': na_percentage,\n",
    "            'Summary': summary,\n",
    "            'Strong Correlations': str(correlations) if correlations else \"None\",\n",
    "            'Recommended as Confounder': recommended,\n",
    "            'Recommendation Reason': \"; \".join(recommendation_reason)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def analyze_target_with_confounders(eigensp_df, metadata_df, stats_df):\n",
    "    \"\"\"\n",
    "    分析每个目标因素与eigenspecies的关系，同时控制其他混杂因素\n",
    "    \n",
    "    参数:\n",
    "    eigensp_df: DataFrame，包含模块和eigenspecies信息，至少包含'module', 'sample', 'eigensp'列\n",
    "    metadata_df: DataFrame，包含元数据信息\n",
    "    stats_df: DataFrame，包含元数据统计信息，包含'Column', 'Recommended as Confounder', 'Data Type'列\n",
    "    \n",
    "    返回:\n",
    "    results_df: DataFrame，包含分析结果\n",
    "    \"\"\"\n",
    "    # 获取所有推荐的混杂因素\n",
    "    confounders = stats_df[stats_df[\"Recommended as Confounder\"] == True][\"Column\"].tolist()\n",
    "    \n",
    "    # 获取数值型和分类型混杂因素\n",
    "    numerical_confounders = stats_df[(stats_df[\"Recommended as Confounder\"] == True) & \n",
    "                                    (stats_df[\"Data Type\"] == \"Numerical\")][\"Column\"].tolist()\n",
    "    categorical_confounders = stats_df[(stats_df[\"Recommended as Confounder\"] == True) & \n",
    "                                      (stats_df[\"Data Type\"] == \"Categorical\")][\"Column\"].tolist()\n",
    "    \n",
    "    # 准备存储结果的列表\n",
    "    all_results = []\n",
    "    \n",
    "    # 对每个混杂因素，轮流将其作为目标因素，其余作为控制变量\n",
    "    for target_col in confounders:\n",
    "        # 确定当前目标的数据类型\n",
    "        target_type = stats_df[stats_df[\"Column\"] == target_col][\"Data Type\"].values[0]\n",
    "        \n",
    "        # 排除当前目标，其余作为混杂因素\n",
    "        current_confounders = [col for col in confounders if col != target_col]\n",
    "        \n",
    "        # 对每个模块进行分析\n",
    "        for module in eigensp_df['module'].unique():\n",
    "            module_df = eigensp_df[eigensp_df['module'] == module]\n",
    "            \n",
    "            # 合并eigenspecies和元数据\n",
    "            merged_df = module_df.merge(metadata_df, left_on='sample', right_on='sample_id', how='inner')\n",
    "            \n",
    "            # 准备模型数据，确保包含所有需要的列\n",
    "            model_cols = [target_col] + current_confounders + ['eigensp']\n",
    "            available_cols = [col for col in model_cols if col in merged_df.columns]\n",
    "            \n",
    "            # 检查是否有足够的数据\n",
    "            if len(available_cols) < len(model_cols) - 5:  # 允许缺少少量列\n",
    "                continue\n",
    "                \n",
    "            model_data = merged_df[available_cols].copy()\n",
    "            \n",
    "            # 处理缺失值\n",
    "            model_data = model_data.dropna()\n",
    "            \n",
    "            if len(model_data) < 20:  # 确保有足够的样本\n",
    "                continue\n",
    "            \n",
    "            # 1. 首先计算简单相关系数（未调整混杂因素）\n",
    "            try:\n",
    "                if target_type == \"Numerical\":\n",
    "                    # 数值型目标 - 使用Pearson相关\n",
    "                    simple_corr, simple_p = pearsonr(model_data[target_col], model_data['eigensp'])\n",
    "                else:\n",
    "                    # 分类型目标 - 只计算均值差异\n",
    "                    simple_corr = None\n",
    "                    simple_p = None\n",
    "            except Exception:\n",
    "                simple_corr = None\n",
    "                simple_p = None\n",
    "            \n",
    "            # 2. 使用回归模型控制混杂因素\n",
    "            try:\n",
    "                # 准备分类变量 - 创建哑变量\n",
    "                cat_cols = [col for col in available_cols if col in categorical_confounders]\n",
    "                if cat_cols:\n",
    "                    model_data_encoded = pd.get_dummies(model_data, columns=cat_cols, drop_first=True)\n",
    "                else:\n",
    "                    model_data_encoded = model_data.copy()\n",
    "                \n",
    "                # 为模型准备自变量（特征）和因变量\n",
    "                y = model_data_encoded['eigensp']\n",
    "                X_cols = [col for col in model_data_encoded.columns if col != 'eigensp']\n",
    "                X = model_data_encoded[X_cols]\n",
    "                \n",
    "                # 添加常数项（截距）\n",
    "                X = sm.add_constant(X)\n",
    "                \n",
    "                # 拟合线性模型\n",
    "                model = sm.OLS(y, X).fit()\n",
    "                \n",
    "                # 提取目标变量的系数和p值\n",
    "                if target_type == \"Numerical\":\n",
    "                    # 数值型目标 - 直接获取系数\n",
    "                    coef = model.params.get(target_col, np.nan)\n",
    "                    p_value = model.pvalues.get(target_col, np.nan)\n",
    "                    std_error = model.bse.get(target_col, np.nan)\n",
    "                else:\n",
    "                    # 分类型目标 - 可能有多个哑变量，获取第一个\n",
    "                    target_cols = [col for col in model.params.index if col.startswith(f\"{target_col}_\")]\n",
    "                    if target_cols:\n",
    "                        coef = model.params[target_cols[0]]\n",
    "                        p_value = model.pvalues[target_cols[0]]\n",
    "                        std_error = model.bse[target_cols[0]]\n",
    "                    else:\n",
    "                        coef = np.nan\n",
    "                        p_value = np.nan\n",
    "                        std_error = np.nan\n",
    "                \n",
    "                # 保存结果\n",
    "                result = {\n",
    "                    'module': module,\n",
    "                    'target': target_col,\n",
    "                    'target_type': target_type,\n",
    "                    'sample_size': len(model_data),\n",
    "                    \n",
    "                    # 简单相关结果\n",
    "                    'simple_correlation': simple_corr,\n",
    "                    'simple_p_value': simple_p,\n",
    "                    \n",
    "                    # 调整后的结果\n",
    "                    'adjusted_coefficient': coef,\n",
    "                    'adjusted_p_value': p_value,\n",
    "                    'adjusted_std_error': std_error,\n",
    "                    'adjusted_r2': model.rsquared_adj,\n",
    "                    \n",
    "                    # 记录使用的混杂因素\n",
    "                    'confounders_used': ','.join([col for col in current_confounders if col in available_cols])\n",
    "                }\n",
    "                \n",
    "                all_results.append(result)\n",
    "                \n",
    "            except Exception:\n",
    "                # 静默跳过错误\n",
    "                continue\n",
    "    \n",
    "    # 创建结果DataFrame\n",
    "    if all_results:\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # 应用FDR校正\n",
    "        # 分别对简单p值和调整后p值进行校正\n",
    "        if 'simple_p_value' in results_df.columns and not results_df['simple_p_value'].isna().all():\n",
    "            _, simple_fdr = fdrcorrection(results_df['simple_p_value'].fillna(1))\n",
    "            results_df['simple_fdr_p_value'] = simple_fdr\n",
    "            \n",
    "        if 'adjusted_p_value' in results_df.columns and not results_df['adjusted_p_value'].isna().all():\n",
    "            _, adjusted_fdr = fdrcorrection(results_df['adjusted_p_value'].fillna(1))\n",
    "            results_df['adjusted_fdr_p_value'] = adjusted_fdr\n",
    "        \n",
    "        return results_df\n",
    "    else:\n",
    "        return pd.DataFrame()  # 返回空DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用示例\n",
    "# 1. 首先计算元数据统计信息\n",
    "stats_df = enhanced_column_stats(metadata_df)\n",
    "# \n",
    "# 2. 分析目标因素与eigenspecies的关系\n",
    "# results_df = analyze_target_with_confounders(eigensp_df, metadata_df, stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = analyze_target_with_confounders(eigensp_df, metadata_df, stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>module</th>\n",
       "      <th>target</th>\n",
       "      <th>target_type</th>\n",
       "      <th>sample_size</th>\n",
       "      <th>simple_correlation</th>\n",
       "      <th>simple_p_value</th>\n",
       "      <th>adjusted_coefficient</th>\n",
       "      <th>adjusted_p_value</th>\n",
       "      <th>adjusted_std_error</th>\n",
       "      <th>adjusted_r2</th>\n",
       "      <th>confounders_used</th>\n",
       "      <th>simple_fdr_p_value</th>\n",
       "      <th>adjusted_fdr_p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S1_C16</td>\n",
       "      <td>study_condition</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.976409</td>\n",
       "      <td>2.198904e-03</td>\n",
       "      <td>0.316661</td>\n",
       "      <td>0.163391</td>\n",
       "      <td>disease,age,age_category,gender,country,number...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.557729e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>S2_C5</td>\n",
       "      <td>study_condition</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.498890</td>\n",
       "      <td>1.437527e-03</td>\n",
       "      <td>0.466788</td>\n",
       "      <td>0.132213</td>\n",
       "      <td>disease,age,age_category,gender,country,number...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.557729e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>S6_C3</td>\n",
       "      <td>age</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>387</td>\n",
       "      <td>0.062811</td>\n",
       "      <td>2.176277e-01</td>\n",
       "      <td>0.007155</td>\n",
       "      <td>1.531584e-03</td>\n",
       "      <td>0.002241</td>\n",
       "      <td>0.033773</td>\n",
       "      <td>study_condition,disease,age_category,gender,co...</td>\n",
       "      <td>8.576629e-01</td>\n",
       "      <td>4.557729e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>S1_C20</td>\n",
       "      <td>gender</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.113597</td>\n",
       "      <td>1.020450e-04</td>\n",
       "      <td>0.792642</td>\n",
       "      <td>0.167277</td>\n",
       "      <td>study_condition,disease,age,age_category,count...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>7.755419e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>S6_C1</td>\n",
       "      <td>gender</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.353672</td>\n",
       "      <td>2.139037e-03</td>\n",
       "      <td>0.114388</td>\n",
       "      <td>0.033993</td>\n",
       "      <td>study_condition,disease,age,age_category,count...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.557729e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>S1_C1</td>\n",
       "      <td>country</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.559894</td>\n",
       "      <td>3.069237e-03</td>\n",
       "      <td>0.523407</td>\n",
       "      <td>0.162111</td>\n",
       "      <td>study_condition,disease,age,age_category,gende...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.998472e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>S1_C8</td>\n",
       "      <td>minimum_read_length</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.286592</td>\n",
       "      <td>3.057447e-03</td>\n",
       "      <td>0.431529</td>\n",
       "      <td>0.053819</td>\n",
       "      <td>study_condition,disease,age,age_category,gende...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.998472e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>S1_C10</td>\n",
       "      <td>mgs_richness</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>387</td>\n",
       "      <td>-0.160683</td>\n",
       "      <td>1.516945e-03</td>\n",
       "      <td>-0.002358</td>\n",
       "      <td>1.997803e-03</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.029540</td>\n",
       "      <td>study_condition,disease,age,age_category,gende...</td>\n",
       "      <td>2.882195e-02</td>\n",
       "      <td>4.557729e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>S1_C2</td>\n",
       "      <td>mgs_richness</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>387</td>\n",
       "      <td>0.197530</td>\n",
       "      <td>9.156617e-05</td>\n",
       "      <td>0.015322</td>\n",
       "      <td>2.043205e-03</td>\n",
       "      <td>0.004933</td>\n",
       "      <td>0.072938</td>\n",
       "      <td>study_condition,disease,age,age_category,gende...</td>\n",
       "      <td>2.609636e-03</td>\n",
       "      <td>4.557729e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>S1_C20</td>\n",
       "      <td>mgs_richness</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>387</td>\n",
       "      <td>0.332334</td>\n",
       "      <td>1.969769e-11</td>\n",
       "      <td>0.052098</td>\n",
       "      <td>6.011119e-09</td>\n",
       "      <td>0.008748</td>\n",
       "      <td>0.167277</td>\n",
       "      <td>study_condition,disease,age,age_category,gende...</td>\n",
       "      <td>4.278259e-09</td>\n",
       "      <td>6.852676e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>S1_C5</td>\n",
       "      <td>mgs_richness</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>387</td>\n",
       "      <td>-0.327905</td>\n",
       "      <td>3.752859e-11</td>\n",
       "      <td>-0.067871</td>\n",
       "      <td>1.773377e-09</td>\n",
       "      <td>0.010998</td>\n",
       "      <td>0.101031</td>\n",
       "      <td>study_condition,disease,age,age_category,gende...</td>\n",
       "      <td>4.278259e-09</td>\n",
       "      <td>4.043300e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>S1_C8</td>\n",
       "      <td>mgs_richness</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>387</td>\n",
       "      <td>0.135144</td>\n",
       "      <td>7.762847e-03</td>\n",
       "      <td>0.006014</td>\n",
       "      <td>7.076036e-04</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.053819</td>\n",
       "      <td>study_condition,disease,age,age_category,gende...</td>\n",
       "      <td>1.106206e-01</td>\n",
       "      <td>4.033340e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>S1_C9</td>\n",
       "      <td>mgs_richness</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>387</td>\n",
       "      <td>-0.160863</td>\n",
       "      <td>1.498339e-03</td>\n",
       "      <td>-0.003877</td>\n",
       "      <td>2.092813e-03</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.016408</td>\n",
       "      <td>study_condition,disease,age,age_category,gende...</td>\n",
       "      <td>2.882195e-02</td>\n",
       "      <td>4.557729e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>S6_C3</td>\n",
       "      <td>mgs_richness</td>\n",
       "      <td>Numerical</td>\n",
       "      <td>387</td>\n",
       "      <td>-0.166620</td>\n",
       "      <td>1.001072e-03</td>\n",
       "      <td>-0.001391</td>\n",
       "      <td>3.038388e-03</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.033773</td>\n",
       "      <td>study_condition,disease,age,age_category,gende...</td>\n",
       "      <td>2.282443e-02</td>\n",
       "      <td>4.998472e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     module               target  target_type  sample_size  \\\n",
       "4    S1_C16      study_condition  Categorical          387   \n",
       "14    S2_C5      study_condition  Categorical          387   \n",
       "56    S6_C3                  age    Numerical          387   \n",
       "83   S1_C20               gender  Categorical          387   \n",
       "93    S6_C1               gender  Categorical          387   \n",
       "95    S1_C1              country  Categorical          387   \n",
       "164   S1_C8  minimum_read_length  Categorical          387   \n",
       "210  S1_C10         mgs_richness    Numerical          387   \n",
       "215   S1_C2         mgs_richness    Numerical          387   \n",
       "216  S1_C20         mgs_richness    Numerical          387   \n",
       "220   S1_C5         mgs_richness    Numerical          387   \n",
       "221   S1_C8         mgs_richness    Numerical          387   \n",
       "222   S1_C9         mgs_richness    Numerical          387   \n",
       "227   S6_C3         mgs_richness    Numerical          387   \n",
       "\n",
       "     simple_correlation  simple_p_value  adjusted_coefficient  \\\n",
       "4                   NaN             NaN             -0.976409   \n",
       "14                  NaN             NaN             -1.498890   \n",
       "56             0.062811    2.176277e-01              0.007155   \n",
       "83                  NaN             NaN              3.113597   \n",
       "93                  NaN             NaN              0.353672   \n",
       "95                  NaN             NaN              1.559894   \n",
       "164                 NaN             NaN              1.286592   \n",
       "210           -0.160683    1.516945e-03             -0.002358   \n",
       "215            0.197530    9.156617e-05              0.015322   \n",
       "216            0.332334    1.969769e-11              0.052098   \n",
       "220           -0.327905    3.752859e-11             -0.067871   \n",
       "221            0.135144    7.762847e-03              0.006014   \n",
       "222           -0.160863    1.498339e-03             -0.003877   \n",
       "227           -0.166620    1.001072e-03             -0.001391   \n",
       "\n",
       "     adjusted_p_value  adjusted_std_error  adjusted_r2  \\\n",
       "4        2.198904e-03            0.316661     0.163391   \n",
       "14       1.437527e-03            0.466788     0.132213   \n",
       "56       1.531584e-03            0.002241     0.033773   \n",
       "83       1.020450e-04            0.792642     0.167277   \n",
       "93       2.139037e-03            0.114388     0.033993   \n",
       "95       3.069237e-03            0.523407     0.162111   \n",
       "164      3.057447e-03            0.431529     0.053819   \n",
       "210      1.997803e-03            0.000757     0.029540   \n",
       "215      2.043205e-03            0.004933     0.072938   \n",
       "216      6.011119e-09            0.008748     0.167277   \n",
       "220      1.773377e-09            0.010998     0.101031   \n",
       "221      7.076036e-04            0.001761     0.053819   \n",
       "222      2.092813e-03            0.001251     0.016408   \n",
       "227      3.038388e-03            0.000466     0.033773   \n",
       "\n",
       "                                      confounders_used  simple_fdr_p_value  \\\n",
       "4    disease,age,age_category,gender,country,number...        1.000000e+00   \n",
       "14   disease,age,age_category,gender,country,number...        1.000000e+00   \n",
       "56   study_condition,disease,age_category,gender,co...        8.576629e-01   \n",
       "83   study_condition,disease,age,age_category,count...        1.000000e+00   \n",
       "93   study_condition,disease,age,age_category,count...        1.000000e+00   \n",
       "95   study_condition,disease,age,age_category,gende...        1.000000e+00   \n",
       "164  study_condition,disease,age,age_category,gende...        1.000000e+00   \n",
       "210  study_condition,disease,age,age_category,gende...        2.882195e-02   \n",
       "215  study_condition,disease,age,age_category,gende...        2.609636e-03   \n",
       "216  study_condition,disease,age,age_category,gende...        4.278259e-09   \n",
       "220  study_condition,disease,age,age_category,gende...        4.278259e-09   \n",
       "221  study_condition,disease,age,age_category,gende...        1.106206e-01   \n",
       "222  study_condition,disease,age,age_category,gende...        2.882195e-02   \n",
       "227  study_condition,disease,age,age_category,gende...        2.282443e-02   \n",
       "\n",
       "     adjusted_fdr_p_value  \n",
       "4            4.557729e-02  \n",
       "14           4.557729e-02  \n",
       "56           4.557729e-02  \n",
       "83           7.755419e-03  \n",
       "93           4.557729e-02  \n",
       "95           4.998472e-02  \n",
       "164          4.998472e-02  \n",
       "210          4.557729e-02  \n",
       "215          4.557729e-02  \n",
       "216          6.852676e-07  \n",
       "220          4.043300e-07  \n",
       "221          4.033340e-02  \n",
       "222          4.557729e-02  \n",
       "227          4.998472e-02  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[results_df['adjusted_fdr_p_value']<0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['study_condition',\n",
       " 'disease',\n",
       " 'DNA_extraction_kit',\n",
       " 'number_reads',\n",
       " 'number_bases',\n",
       " 'days_from_first_collection',\n",
       " 'location',\n",
       " 'visit_number',\n",
       " 'disease_subtype']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confounders = stats_df[stats_df[\"Recommended as Confounder\"] == True][\"Column\"].tolist()\n",
    "confounders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['number_reads', 'number_bases', 'days_from_first_collection', 'visit_number']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_confounders = stats_df[(stats_df[\"Recommended as Confounder\"] == True) & \n",
    "                                (stats_df[\"Data Type\"] == \"Numerical\")][\"Column\"].tolist()\n",
    "categorical_confounders = stats_df[(stats_df[\"Recommended as Confounder\"] == True) & \n",
    "                                    (stats_df[\"Data Type\"] == \"Categorical\")][\"Column\"].tolist()\n",
    "numerical_confounders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['study_condition',\n",
       " 'disease',\n",
       " 'DNA_extraction_kit',\n",
       " 'location',\n",
       " 'disease_subtype']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_confounders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第1步：导入所需库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 第2步：准备数据 - 假设您已经有了eigensp_df和metadata_df\n",
    "# eigensp_df: 包含'module', 'sample', 'eigensp'列\n",
    "# metadata_df: 包含元数据，包括'disease_subtype'列\n",
    "# 确保这些数据已经正确加载\n",
    "\n",
    "# 第3步：分析指定目标变量与eigenspecies的关系\n",
    "def analyze_specific_target(eigensp_df, metadata_df, target_col, stats_df=None):\n",
    "    \"\"\"\n",
    "    分析特定目标变量与eigenspecies的关系，同时控制其他混杂因素\n",
    "    \n",
    "    参数:\n",
    "    eigensp_df: DataFrame，包含模块和eigenspecies信息\n",
    "    metadata_df: DataFrame，包含元数据\n",
    "    target_col: 要分析的目标变量名\n",
    "    stats_df: 可选，元数据统计DataFrame，如果提供则使用其推荐的混杂因素\n",
    "    \n",
    "    返回:\n",
    "    results_df: DataFrame，包含分析结果\n",
    "    \"\"\"\n",
    "    # 如果提供了stats_df，使用其推荐的混杂因素，否则使用默认列表\n",
    "    if stats_df is not None:\n",
    "        # 获取推荐的混杂因素\n",
    "        numerical_confounders = stats_df[(stats_df[\"Recommended as Confounder\"] == True) & \n",
    "                                        (stats_df[\"Data Type\"] == \"Numerical\")][\"Column\"].tolist()\n",
    "        categorical_confounders = stats_df[(stats_df[\"Recommended as Confounder\"] == True) & \n",
    "                                          (stats_df[\"Data Type\"] == \"Categorical\")][\"Column\"].tolist()\n",
    "        \n",
    "        # 从混杂因素中排除目标变量\n",
    "        if target_col in numerical_confounders:\n",
    "            numerical_confounders.remove(target_col)\n",
    "        if target_col in categorical_confounders:\n",
    "            categorical_confounders.remove(target_col)\n",
    "    else:\n",
    "        # 使用默认的混杂因素列表 - 可根据您的数据调整\n",
    "        numerical_confounders = ['age', 'BMI', 'number_reads', 'number_bases', 'mgs_richness']\n",
    "        categorical_confounders = ['gender', 'country', 'non_westernized', 'sequencing_platform']\n",
    "    \n",
    "    # 打印使用的混杂因素\n",
    "    print(f\"Target variable: {target_col}\")\n",
    "    print(f\"Numerical confounders ({len(numerical_confounders)}): {', '.join(numerical_confounders)}\")\n",
    "    print(f\"Categorical confounders ({len(categorical_confounders)}): {', '.join(categorical_confounders)}\")\n",
    "    \n",
    "    # 确定目标变量的类型\n",
    "    if target_col in metadata_df.columns:\n",
    "        # 检查目标变量是数值型还是分类型\n",
    "        if pd.api.types.is_numeric_dtype(metadata_df[target_col]) and metadata_df[target_col].nunique() > 10:\n",
    "            target_type = \"Numerical\"\n",
    "        else:\n",
    "            target_type = \"Categorical\"\n",
    "        print(f\"Target type detected: {target_type}\")\n",
    "    else:\n",
    "        print(f\"Error: Target column '{target_col}' not found in metadata\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # 所有混杂因素的列表\n",
    "    all_confounders = numerical_confounders + categorical_confounders\n",
    "    \n",
    "    # 准备存储结果的列表\n",
    "    all_results = []\n",
    "    \n",
    "    # 对每个模块进行分析\n",
    "    for module in eigensp_df['module'].unique():\n",
    "        print(f\"\\nAnalyzing module: {module}\")\n",
    "        \n",
    "        # 获取该模块的eigenspecies数据\n",
    "        module_df = eigensp_df[eigensp_df['module'] == module]\n",
    "        print(f\"  Module samples: {len(module_df)}\")\n",
    "        \n",
    "        # 合并eigenspecies和元数据\n",
    "        merged_df = module_df.merge(metadata_df, left_on='sample', right_on='sample_id', how='inner')\n",
    "        print(f\"  Merged samples: {len(merged_df)}\")\n",
    "        \n",
    "        # 准备模型数据，包含目标变量、混杂因素和eigensp\n",
    "        model_cols = [target_col] + all_confounders + ['eigensp']\n",
    "        available_cols = [col for col in model_cols if col in merged_df.columns]\n",
    "        missing_cols = [col for col in model_cols if col not in merged_df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"  Missing columns: {', '.join(missing_cols)}\")\n",
    "        \n",
    "        if target_col not in available_cols:\n",
    "            print(f\"  Error: Target column '{target_col}' not available after merge\")\n",
    "            continue\n",
    "            \n",
    "        # 创建工作数据集的副本\n",
    "        model_data = merged_df[available_cols].copy()\n",
    "        \n",
    "        # 处理缺失值\n",
    "        before_dropna = len(model_data)\n",
    "        model_data = model_data.dropna()\n",
    "        after_dropna = len(model_data)\n",
    "        \n",
    "        print(f\"  Complete cases: {after_dropna}/{before_dropna} ({after_dropna/before_dropna*100:.1f}%)\")\n",
    "        \n",
    "        if after_dropna < 20:  # 确保有足够的样本\n",
    "            print(f\"  Warning: Not enough complete cases ({after_dropna}), skipping module\")\n",
    "            continue\n",
    "        \n",
    "        # 步骤4：计算简单相关（未调整混杂因素）\n",
    "        try:\n",
    "            if target_type == \"Numerical\":\n",
    "                # 数值型目标 - 使用Pearson相关\n",
    "                simple_corr, simple_p = pearsonr(model_data[target_col], model_data['eigensp'])\n",
    "                print(f\"  Simple correlation: r={simple_corr:.3f}, p={simple_p:.5f}\")\n",
    "            else:\n",
    "                # 分类型目标 - 展示每个类别的均值\n",
    "                print(\"  Category means:\")\n",
    "                for category, group in model_data.groupby(target_col):\n",
    "                    if len(group) >= 5:  # 只显示有足够样本的类别\n",
    "                        print(f\"    {category}: mean={group['eigensp'].mean():.3f}, n={len(group)}\")\n",
    "                simple_corr = None\n",
    "                simple_p = None\n",
    "        except Exception as e:\n",
    "            print(f\"  Error calculating simple correlation: {e}\")\n",
    "            simple_corr = None\n",
    "            simple_p = None\n",
    "        \n",
    "        # 步骤5：使用回归模型控制混杂因素\n",
    "        try:\n",
    "            # 获取当前可用的混杂因素\n",
    "            avail_num_conf = [col for col in numerical_confounders if col in available_cols]\n",
    "            avail_cat_conf = [col for col in categorical_confounders if col in available_cols]\n",
    "            \n",
    "            print(f\"  Available numerical confounders: {len(avail_num_conf)}\")\n",
    "            print(f\"  Available categorical confounders: {len(avail_cat_conf)}\")\n",
    "            \n",
    "            # 准备分类变量 - 创建哑变量\n",
    "            cat_cols = avail_cat_conf\n",
    "            if target_type == \"Categorical\":\n",
    "                cat_cols.append(target_col)\n",
    "                \n",
    "            if cat_cols:\n",
    "                print(f\"  Creating dummy variables for: {', '.join(cat_cols)}\")\n",
    "                model_data_encoded = pd.get_dummies(model_data, columns=cat_cols, drop_first=True)\n",
    "            else:\n",
    "                model_data_encoded = model_data.copy()\n",
    "            \n",
    "            # 显示编码后的列数\n",
    "            print(f\"  Columns after encoding: {len(model_data_encoded.columns)}\")\n",
    "            \n",
    "            # 为模型准备自变量（特征）和因变量\n",
    "            y = model_data_encoded['eigensp']\n",
    "            X_cols = [col for col in model_data_encoded.columns if col != 'eigensp']\n",
    "            X = model_data_encoded[X_cols]\n",
    "            \n",
    "            # 添加常数项（截距）\n",
    "            X = sm.add_constant(X)\n",
    "            \n",
    "            # 拟合线性模型\n",
    "            print(\"  Fitting linear model...\")\n",
    "            model = sm.OLS(y, X).fit()\n",
    "            \n",
    "            # 提取结果\n",
    "            print(f\"  Model summary: R²={model.rsquared:.3f}, Adj. R²={model.rsquared_adj:.3f}\")\n",
    "            \n",
    "            # 提取目标变量的系数和p值\n",
    "            if target_type == \"Numerical\":\n",
    "                # 数值型目标 - 直接获取系数\n",
    "                coef = model.params.get(target_col, np.nan)\n",
    "                p_value = model.pvalues.get(target_col, np.nan)\n",
    "                std_error = model.bse.get(target_col, np.nan)\n",
    "                \n",
    "                print(f\"  Adjusted result: coef={coef:.3f}, p={p_value:.5f}, se={std_error:.3f}\")\n",
    "            else:\n",
    "                # 分类型目标 - 可能有多个哑变量\n",
    "                target_cols = [col for col in model.params.index if col.startswith(f\"{target_col}_\")]\n",
    "                print(f\"  Target dummy variables: {len(target_cols)}\")\n",
    "                \n",
    "                # 显示每个类别的系数\n",
    "                for col in target_cols:\n",
    "                    coef = model.params.get(col, np.nan)\n",
    "                    p_value = model.pvalues.get(col, np.nan)\n",
    "                    std_error = model.bse.get(col, np.nan)\n",
    "                    \n",
    "                    category = col.replace(f\"{target_col}_\", \"\")\n",
    "                    print(f\"    {category}: coef={coef:.3f}, p={p_value:.5f}, se={std_error:.3f}\")\n",
    "                \n",
    "                # 使用第一个哑变量的结果（如果有）\n",
    "                if target_cols:\n",
    "                    coef = model.params[target_cols[0]]\n",
    "                    p_value = model.pvalues[target_cols[0]]\n",
    "                    std_error = model.bse[target_cols[0]]\n",
    "                    dummy_var = target_cols[0]\n",
    "                else:\n",
    "                    coef = np.nan\n",
    "                    p_value = np.nan\n",
    "                    std_error = np.nan\n",
    "                    dummy_var = None\n",
    "            \n",
    "            # 保存结果\n",
    "            result = {\n",
    "                'module': module,\n",
    "                'target': target_col,\n",
    "                'target_type': target_type,\n",
    "                'sample_size': len(model_data),\n",
    "                \n",
    "                # 简单相关结果\n",
    "                'simple_correlation': simple_corr,\n",
    "                'simple_p_value': simple_p,\n",
    "                \n",
    "                # 调整后的结果\n",
    "                'adjusted_coefficient': coef,\n",
    "                'adjusted_p_value': p_value,\n",
    "                'adjusted_std_error': std_error,\n",
    "                'adjusted_r2': model.rsquared_adj,\n",
    "                \n",
    "                # 记录使用的混杂因素\n",
    "                'numerical_confounders': ','.join(avail_num_conf),\n",
    "                'categorical_confounders': ','.join(avail_cat_conf)\n",
    "            }\n",
    "            \n",
    "            # 如果是分类变量，添加使用的哑变量信息\n",
    "            if target_type == \"Categorical\" and dummy_var:\n",
    "                result['dummy_variable'] = dummy_var\n",
    "            \n",
    "            all_results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error in regression model: {e}\")\n",
    "    \n",
    "    # 步骤6：创建结果DataFrame\n",
    "    if all_results:\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        print(\"\\nAnalysis complete. Found results for:\")\n",
    "        \n",
    "        # 显示每个模块的结果\n",
    "        for module, group in results_df.groupby('module'):\n",
    "            significant = sum(group['adjusted_p_value'] < 0.05)\n",
    "            print(f\"  Module {module}: {len(group)} results, {significant} significant (p<0.05)\")\n",
    "        \n",
    "        # 应用FDR校正\n",
    "        if 'adjusted_p_value' in results_df.columns and not results_df['adjusted_p_value'].isna().all():\n",
    "            _, adjusted_fdr = fdrcorrection(results_df['adjusted_p_value'].fillna(1))\n",
    "            results_df['adjusted_fdr_p_value'] = adjusted_fdr\n",
    "            \n",
    "            significant_fdr = sum(results_df['adjusted_fdr_p_value'] < 0.05)\n",
    "            print(f\"\\nAfter FDR correction: {significant_fdr} significant results (q<0.05)\")\n",
    "        \n",
    "        return results_df\n",
    "    else:\n",
    "        print(\"\\nNo valid results found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# 步骤7：运行分析\n",
    "# 假设stats_df已经由enhanced_column_stats函数生成\n",
    "# results_df = analyze_specific_target(eigensp_df, metadata_df, 'disease_subtype', stats_df)\n",
    "\n",
    "# 步骤8：查看显著结果\n",
    "# significant_results = results_df[results_df['adjusted_fdr_p_value'] < 0.05].sort_values('adjusted_fdr_p_value')\n",
    "# display(significant_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable: disease_subtype\n",
      "Numerical confounders (4): number_reads, number_bases, days_from_first_collection, visit_number\n",
      "Categorical confounders (4): study_condition, disease, DNA_extraction_kit, location\n",
      "Target type detected: Categorical\n",
      "\n",
      "Analyzing module: S1_C1\n",
      "  Module samples: 393\n",
      "  Merged samples: 0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_specific_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43meigensp_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdisease_subtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 102\u001b[0m, in \u001b[0;36manalyze_specific_target\u001b[0;34m(eigensp_df, metadata_df, target_col, stats_df)\u001b[0m\n\u001b[1;32m     99\u001b[0m model_data \u001b[38;5;241m=\u001b[39m model_data\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m    100\u001b[0m after_dropna \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(model_data)\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Complete cases: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mafter_dropna\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbefore_dropna\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mafter_dropna\u001b[38;5;241m/\u001b[39mbefore_dropna\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m after_dropna \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m20\u001b[39m:  \u001b[38;5;66;03m# 确保有足够的样本\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Warning: Not enough complete cases (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mafter_dropna\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), skipping module\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "results_df = analyze_specific_target(eigensp_df, metadata_df, 'disease_subtype', stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigensp_df sample IDs (前5个): ['MH0001', 'MH0002', 'MH0003', 'MH0004', 'MH0005']\n",
      "metadata_df sample IDs (前5个): ['SKST006_6_G102964', 'SKST006_7_G102965', 'SKST006_4_G102962', 'SKST006_5_G102963', 'SKST006_2_G102960']\n"
     ]
    }
   ],
   "source": [
    "print(\"eigensp_df sample IDs (前5个):\", eigensp_df['sample'].iloc[:5].tolist())\n",
    "print(\"metadata_df sample IDs (前5个):\", metadata_df['sample_id'].iloc[:5].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_samples = set(eigensp_df['sample']).intersection(set(metadata_df['sample_id']))\n",
    "common_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          MH0001\n",
       "1          MH0002\n",
       "2          MH0003\n",
       "3          MH0004\n",
       "4          MH0005\n",
       "          ...    \n",
       "7462    V1_UC54_0\n",
       "7463    V1_UC55_0\n",
       "7464    V1_UC55_4\n",
       "7465    V1_UC56_0\n",
       "7466    V1_UC58_0\n",
       "Name: sample, Length: 7467, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigensp_df['sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SKST006_10_G102994',\n",
       " 'SKST006_1_G102959',\n",
       " 'SKST006_2_G102960',\n",
       " 'SKST006_3_G102961',\n",
       " 'SKST006_4_G102962',\n",
       " 'SKST006_5_G102963',\n",
       " 'SKST006_6_G102964',\n",
       " 'SKST006_7_G102965',\n",
       " 'SKST006_9_G103014',\n",
       " 'SKST007_1_G102966',\n",
       " 'SKST007_2_G102967',\n",
       " 'SKST007_3_G102949',\n",
       " 'SKST007_4_G102948',\n",
       " 'SKST007_6_G102995',\n",
       " 'SKST007_7_G103016',\n",
       " 'SKST007_8_G102999',\n",
       " 'SKST010_1_G102968',\n",
       " 'SKST010_2_G102969',\n",
       " 'SKST010_3_G102970',\n",
       " 'SKST010_4_G102956',\n",
       " 'SKST010_5_G102990',\n",
       " 'SKST010_6_G103004',\n",
       " 'SKST010_7_G103003',\n",
       " 'SKST010_8_G102998',\n",
       " 'SKST011_1_G102971',\n",
       " 'SKST011_2_G102972',\n",
       " 'SKST011_3_G102973',\n",
       " 'SKST011_4_G102952',\n",
       " 'SKST011_5_G102993',\n",
       " 'SKST011_6_G103002',\n",
       " 'SKST011_7_G103017',\n",
       " 'SKST011_8_G103000',\n",
       " 'SKST012_1_G102974',\n",
       " 'SKST012_2_G102975',\n",
       " 'SKST012_3_G102950',\n",
       " 'SKST012_4_G102954',\n",
       " 'SKST012_5_G102992',\n",
       " 'SKST012_6_G102997',\n",
       " 'SKST014_1_G102976',\n",
       " 'SKST014_2_G102977',\n",
       " 'SKST014_3_G102978',\n",
       " 'SKST014_4_G102979',\n",
       " 'SKST014_5_G102980',\n",
       " 'SKST014_6_G102951',\n",
       " 'SKST014_7_G102940',\n",
       " 'SKST023_1_G102981',\n",
       " 'SKST023_2_G102987',\n",
       " 'SKST023_3_G102941',\n",
       " 'SKST023_4_G102937',\n",
       " 'SKST023_5_G103007',\n",
       " 'SKST023_6_G103009',\n",
       " 'SKST023_7_G103015',\n",
       " 'SKST024_1_G102982',\n",
       " 'SKST024_2_G102953',\n",
       " 'SKST024_3_G102944',\n",
       " 'SKST024_4_G102938',\n",
       " 'SKST024_5_G103001',\n",
       " 'SKST024_6_G103012',\n",
       " 'SKST024_7_G103022',\n",
       " 'SKST024_8_G103031',\n",
       " 'SKST025_1_G102983',\n",
       " 'SKST025_2_G102986',\n",
       " 'SKST025_3_G102942',\n",
       " 'SKST025_4_G102939',\n",
       " 'SKST025_5_G103006',\n",
       " 'SKST025_6_G103010',\n",
       " 'SKST025_9_G103021',\n",
       " 'SKST027_10_G102936',\n",
       " 'SKST027_11_G102996',\n",
       " 'SKST027_12_G103011',\n",
       " 'SKST027_1_G102984',\n",
       " 'SKST027_2_G102985',\n",
       " 'SKST027_3_G102945',\n",
       " 'SKST027_4_G102958',\n",
       " 'SKST027_6_G102955',\n",
       " 'SKST027_9_G102947',\n",
       " 'SKST032_1_G102943',\n",
       " 'SKST032_2_G102946',\n",
       " 'SKST032_3_G102989',\n",
       " 'SKST032_4_G103008',\n",
       " 'SKST032_5_G103024',\n",
       " 'SKST036_1_G103013',\n",
       " 'SKST036_2_G103020',\n",
       " 'SKST036_3_G103025',\n",
       " 'SKST037_2_G103023',\n",
       " 'SKST037_3_G103026',\n",
       " 'SKST041_1_G103005',\n",
       " 'SKST041_2_G103027',\n",
       " 'SKST041_3_G103028',\n",
       " 'p8582_mo1',\n",
       " 'p8582_mo10',\n",
       " 'p8582_mo11',\n",
       " 'p8582_mo12',\n",
       " 'p8582_mo2',\n",
       " 'p8582_mo3',\n",
       " 'p8582_mo4',\n",
       " 'p8582_mo5',\n",
       " 'p8582_mo6',\n",
       " 'p8582_mo7',\n",
       " 'p8582_mo8',\n",
       " 'p8582_mo9',\n",
       " 'p8585_mo1',\n",
       " 'p8585_mo2',\n",
       " 'p8585_mo3',\n",
       " 'p8585_mo4',\n",
       " 'p8585_mo6',\n",
       " 'p8585_mo7',\n",
       " 'p8585_mo8',\n",
       " 'p8585_mo9',\n",
       " 'p8600_mo1',\n",
       " 'p8600_mo10',\n",
       " 'p8600_mo11',\n",
       " 'p8600_mo12',\n",
       " 'p8600_mo2',\n",
       " 'p8600_mo3',\n",
       " 'p8600_mo4',\n",
       " 'p8600_mo5',\n",
       " 'p8600_mo6',\n",
       " 'p8600_mo7',\n",
       " 'p8600_mo8',\n",
       " 'p8600_mo9',\n",
       " 'p8646_mo1',\n",
       " 'p8646_mo10',\n",
       " 'p8646_mo11',\n",
       " 'p8646_mo12',\n",
       " 'p8646_mo2',\n",
       " 'p8646_mo3',\n",
       " 'p8646_mo4',\n",
       " 'p8646_mo5',\n",
       " 'p8646_mo6',\n",
       " 'p8646_mo7',\n",
       " 'p8646_mo8',\n",
       " 'p8646_mo9',\n",
       " 'p8649_mo1',\n",
       " 'p8649_mo10',\n",
       " 'p8649_mo11',\n",
       " 'p8649_mo12',\n",
       " 'p8649_mo2',\n",
       " 'p8649_mo3',\n",
       " 'p8649_mo4',\n",
       " 'p8649_mo5',\n",
       " 'p8649_mo6',\n",
       " 'p8649_mo7',\n",
       " 'p8649_mo8',\n",
       " 'p8649_mo9',\n",
       " 'p8712_mo1',\n",
       " 'p8712_mo10',\n",
       " 'p8712_mo11',\n",
       " 'p8712_mo12',\n",
       " 'p8712_mo2',\n",
       " 'p8712_mo3',\n",
       " 'p8712_mo4',\n",
       " 'p8712_mo5',\n",
       " 'p8712_mo6',\n",
       " 'p8712_mo7',\n",
       " 'p8712_mo8',\n",
       " 'p8712_mo9',\n",
       " 'p8748_mo10',\n",
       " 'p8748_mo11',\n",
       " 'p8748_mo12',\n",
       " 'p8748_mo4',\n",
       " 'p8748_mo5',\n",
       " 'p8748_mo6',\n",
       " 'p8748_mo7',\n",
       " 'p8748_mo8',\n",
       " 'p8748_mo9',\n",
       " 'p8775_mo1',\n",
       " 'p8775_mo10',\n",
       " 'p8775_mo11',\n",
       " 'p8775_mo12',\n",
       " 'p8775_mo2',\n",
       " 'p8775_mo3',\n",
       " 'p8775_mo4',\n",
       " 'p8775_mo5',\n",
       " 'p8775_mo6',\n",
       " 'p8775_mo7',\n",
       " 'p8775_mo8',\n",
       " 'p8775_mo9',\n",
       " 'p8808_mo10',\n",
       " 'p8808_mo11',\n",
       " 'p8808_mo12',\n",
       " 'p8808_mo4',\n",
       " 'p8808_mo5',\n",
       " 'p8808_mo6',\n",
       " 'p8808_mo7',\n",
       " 'p8808_mo8',\n",
       " 'p8808_mo9',\n",
       " 'p8816_mo1',\n",
       " 'p8816_mo10',\n",
       " 'p8816_mo11',\n",
       " 'p8816_mo12',\n",
       " 'p8816_mo2',\n",
       " 'p8816_mo3',\n",
       " 'p8816_mo4',\n",
       " 'p8816_mo5',\n",
       " 'p8816_mo6',\n",
       " 'p8816_mo7',\n",
       " 'p8816_mo8',\n",
       " 'p8816_mo9',\n",
       " 'p8855_mo10',\n",
       " 'p8855_mo11',\n",
       " 'p8855_mo12',\n",
       " 'p8855_mo3',\n",
       " 'p8855_mo4',\n",
       " 'p8855_mo8',\n",
       " 'p8855_mo9',\n",
       " 'p8883_mo10',\n",
       " 'p8883_mo11',\n",
       " 'p8883_mo12',\n",
       " 'p8883_mo2',\n",
       " 'p8883_mo3',\n",
       " 'p8883_mo4',\n",
       " 'p8883_mo6',\n",
       " 'p8883_mo7',\n",
       " 'p8883_mo8',\n",
       " 'p8883_mo9',\n",
       " 'p9061_mo1',\n",
       " 'p9061_mo2',\n",
       " 'p9061_mo3',\n",
       " 'p9061_mo4',\n",
       " 'p9061_mo5',\n",
       " 'p9061_mo6',\n",
       " 'p9061_mo7',\n",
       " 'p9061_mo8',\n",
       " 'p9193_mo1',\n",
       " 'p9193_mo2',\n",
       " 'p9193_mo4',\n",
       " 'p9193_mo5',\n",
       " 'p9193_mo6',\n",
       " 'p9193_mo7',\n",
       " 'p9216_mo1',\n",
       " 'p9216_mo2',\n",
       " 'p9216_mo3',\n",
       " 'p9216_mo4',\n",
       " 'p9216_mo5',\n",
       " 'p9216_mo6',\n",
       " 'p9216_mo7',\n",
       " 'p9216_mo8',\n",
       " 'p9220_mo1',\n",
       " 'p9220_mo2',\n",
       " 'p9220_mo3',\n",
       " 'p9220_mo4',\n",
       " 'p9220_mo5',\n",
       " 'p9220_mo6',\n",
       " 'p9220_mo7',\n",
       " 'p9223_mo1',\n",
       " 'p9223_mo2',\n",
       " 'p9223_mo3',\n",
       " 'p9223_mo4',\n",
       " 'p9223_mo5',\n",
       " 'p9223_mo6',\n",
       " 'p9223_mo7',\n",
       " 'p9223_mo8',\n",
       " 'p9281_mo1',\n",
       " 'p9281_mo2',\n",
       " 'p9281_mo3',\n",
       " 'p9281_mo4',\n",
       " 'p9281_mo5',\n",
       " 'p9281_mo6'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(metadata_df['sample_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta_fr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
